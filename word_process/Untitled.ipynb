{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Dinamo_Kharkiv_Vibrani_virshi.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Сергій Жадан \\nДинамо Харків. Вибрані вірші\\n     \\n\\n     \\nСергій Жадан\\nДинамо Харків. Вибрані вірші\\n     \\n     \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 66,  82,  93,  80, 107,  86,   2,  56,  77,  81,  77,  90,   2,\n",
       "         0,  54,  85,  90,  77,  89,  91,   2,  70,  77,  93,  87, 107,\n",
       "        79,   9,   2,  52,  85,  78,  93,  77,  90, 107,   2,  79, 107,\n",
       "        93, 101, 107,   0,   2,   2,   2,   2,   2,   0,   0,   2,   2,\n",
       "         2,   2,   2,   0,  66,  82,  93,  80, 107,  86,   2,  56,  77,\n",
       "        81,  77,  90,   0,  54,  85,  90,  77,  89,  91,   2,  70,  77,\n",
       "        93,  87, 107,  79,   9,   2,  52,  85,  78,  93,  77,  90, 107,\n",
       "         2,  79, 107,  93, 101, 107,   0,   2,   2,   2,   2,   2,   0,\n",
       "         2,   2,   2,   2,   2,   0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Создаем генератор, который возвращает пакеты размером\n",
    "       n_seqs x n_steps из массива arr.\n",
    "       \n",
    "       Аргументы\n",
    "       ---------\n",
    "       arr: Массив, из которого получаем пакеты\n",
    "       n_seqs: Batch size, количество последовательностей в пакете\n",
    "       n_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
    "    '''\n",
    "    # Считаем количество символов на пакет и количество пакетов, которое можем сформировать\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    # Сохраняем в массиве только символы, которые позволяют сформировать целое число пакетов\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Делаем reshape 1D -> 2D, используя n_seqs как число строк, как на картинке\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # пакет данных, который будет подаваться на вход сети\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # целевой пакет, с которым будем сравнивать предсказание, получаем сдвиганием \"x\" на один символ вперед\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 66  82  93  80 107]\n",
      " [ 95  22  93 107  79]\n",
      " [  2   2 107   2  79]\n",
      " [ 95  85  88  85   2]\n",
      " [  0   2   2   2   2]]\n",
      "\n",
      "y\n",
      " [[ 82  93  80 107  86]\n",
      " [ 22  93 107  79   8]\n",
      " [  2 107   2  79  77]\n",
      " [ 85  88  85   2  90]\n",
      " [  2   2   2   2   2]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:5, :5])\n",
    "print('\\ny\\n', y[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Определяем placeholder'ы для входных, целевых данных, а также вероятности drop out\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        batch_size: Batch size, количество последовательностей в пакете\n",
    "        num_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
    "        \n",
    "    '''\n",
    "    # Объявляем placeholder'ы\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Placeholder для вероятности drop out\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Строим LSTM ячейку.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        keep_prob: Скаляр (tf.placeholder) для dropout keep probability\n",
    "        lstm_size: Размер скрытых слоев в LSTM ячейках\n",
    "        num_layers: Количество LSTM слоев\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Строим LSTM ячейку\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Начинаем с базовой LSTM ячейки\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Добавляем dropout к ячейке\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Стэкируем несколько LSTM слоев для придания глубины нашему deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    # Инициализируем начальное состояние LTSM ячейки\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Строим softmax слой и возвращаем результат его работы.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        \n",
    "        x: Входящий от LSTM тензор\n",
    "        in_size: Размер входящего тензора, (кол-во LSTM юнитов скрытого слоя)\n",
    "        out_size: Размер softmax слоя (объем словаря)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # вытягиваем и решэйпим тензор, выполняя преобразование 3D -> 2D\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Соединяем результат LTSM слоев с softmax слоем\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Считаем logit-функцию\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    # Используем функцию softmax для получения предсказания\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Считаем функцию потери на основании значений logit-функции и целевых значений.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        logits: значение logit-функции\n",
    "        targets: целевые значения, с которыми сравниваем предсказания\n",
    "        lstm_size: Количество юнитов в LSTM слое\n",
    "        num_classes: Количество классов в целевых значениях (размер словаря)\n",
    "        \n",
    "    '''\n",
    "    # Делаем one-hot кодирование целевых значений и решейпим по образу и подобию logits\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Считаем значение функции потери softmax cross entropy loss и возвращаем среднее значение\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Строим оптимизатор для обучения, используя обрезку градиента.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: значение функции потери\n",
    "        learning_rate: параметр скорости обучения\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Оптимизатор для обучения, обрезка градиента для контроля \"взрывающихся\" градиентов\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # Мы будем использовать эту же сеть для сэмплирования (генерации текста),\n",
    "        # при этом будем подавать по одному символу за один раз\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Получаем input placeholder'ы\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Строим LSTM ячейку\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Прогоняем данные через RNN слои\n",
    "        # Делаем one-hot кодирование входящих данных\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Прогоняем данные через RNN и собираем результаты\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Получаем предсказания (softmax) и результат logit-функции\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Считаем потери и оптимизируем (с обрезкой градиента)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50        # Размер пакета\n",
    "num_steps = 50         # Шагов в пакете\n",
    "lstm_size = 512         # Количество LSTM юнитов в скрытом слое\n",
    "num_layers = 2          # Количество LSTM слоев\n",
    "learning_rate = 0.001   # Скорость обучения\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-d79244e01a4f>:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.7518...  3.4134 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.6222...  3.7337 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.9887...  3.3194 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 7.2948...  3.3844 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 5.6346...  3.5175 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.5640...  3.4214 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.7446...  3.8107 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.5898...  5.5650 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.5120...  4.5823 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.3803...  3.8117 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.4044...  3.9898 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.3603...  3.8027 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3171...  4.6383 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3369...  4.2740 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.2485...  4.2280 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.2236...  4.1550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.2895...  4.1229 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.2713...  4.0369 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2457...  3.9468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2218...  4.6583 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2479...  4.7794 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2394...  4.2200 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2384...  4.1329 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2344...  4.5262 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.1788...  4.4572 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2351...  3.8748 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.1774...  3.9458 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2316...  4.1429 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.1965...  4.0519 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.1837...  4.0329 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2121...  4.3341 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.1604...  6.4006 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.1699...  4.9075 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.1422...  4.1219 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.2471...  5.5890 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.1959...  4.5903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1433...  5.3778 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1585...  5.0466 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1511...  5.0876 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1077...  4.9505 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1598...  4.9175 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1082...  6.3855 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1488...  4.9405 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.0980...  4.4442 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1071...  5.9662 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1425...  5.7921 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1229...  4.9605 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1357...  5.7281 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.0769...  6.1654 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.0905...  7.0520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.0655...  6.5547 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1264...  5.4689 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.0991...  5.7071 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.0741...  4.7664 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1169...  5.0066 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.0858...  4.6413 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.0337...  6.1634 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.0875...  4.7003 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.0197...  5.2508 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.0539...  4.4722 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 2.9995...  4.8194 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.0543...  4.5512 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.0715...  4.7103 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.0101...  5.8612 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.0454...  5.0106 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.0711...  5.0276 sec/batch\n",
      "Epoch: 2/20...  Training Step: 67...  Training loss: 3.1903...  4.1499 sec/batch\n",
      "Epoch: 2/20...  Training Step: 68...  Training loss: 3.0431...  4.6663 sec/batch\n",
      "Epoch: 2/20...  Training Step: 69...  Training loss: 2.9704...  6.6227 sec/batch\n",
      "Epoch: 2/20...  Training Step: 70...  Training loss: 2.9771...  5.9712 sec/batch\n",
      "Epoch: 2/20...  Training Step: 71...  Training loss: 2.9709...  5.7401 sec/batch\n",
      "Epoch: 2/20...  Training Step: 72...  Training loss: 2.9540...  5.3728 sec/batch\n",
      "Epoch: 2/20...  Training Step: 73...  Training loss: 3.0023...  5.9332 sec/batch\n",
      "Epoch: 2/20...  Training Step: 74...  Training loss: 2.9775...  4.5622 sec/batch\n",
      "Epoch: 2/20...  Training Step: 75...  Training loss: 2.9996...  4.8555 sec/batch\n",
      "Epoch: 2/20...  Training Step: 76...  Training loss: 2.9512...  5.6590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 77...  Training loss: 3.0116...  5.8021 sec/batch\n",
      "Epoch: 2/20...  Training Step: 78...  Training loss: 2.9440...  7.0100 sec/batch\n",
      "Epoch: 2/20...  Training Step: 79...  Training loss: 2.9397...  7.1461 sec/batch\n",
      "Epoch: 2/20...  Training Step: 80...  Training loss: 2.9821...  6.2324 sec/batch\n",
      "Epoch: 2/20...  Training Step: 81...  Training loss: 2.9234...  6.1003 sec/batch\n",
      "Epoch: 2/20...  Training Step: 82...  Training loss: 2.8856...  5.4349 sec/batch\n",
      "Epoch: 2/20...  Training Step: 83...  Training loss: 2.9433...  6.0803 sec/batch\n",
      "Epoch: 2/20...  Training Step: 84...  Training loss: 2.9746...  7.0470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 85...  Training loss: 2.9313...  5.3248 sec/batch\n",
      "Epoch: 2/20...  Training Step: 86...  Training loss: 2.9149...  5.0896 sec/batch\n",
      "Epoch: 2/20...  Training Step: 87...  Training loss: 2.9071...  4.7614 sec/batch\n",
      "Epoch: 2/20...  Training Step: 88...  Training loss: 2.9049...  4.2550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 89...  Training loss: 2.9301...  4.3151 sec/batch\n",
      "Epoch: 2/20...  Training Step: 90...  Training loss: 2.9369...  4.7534 sec/batch\n",
      "Epoch: 2/20...  Training Step: 91...  Training loss: 2.8614...  4.2720 sec/batch\n",
      "Epoch: 2/20...  Training Step: 92...  Training loss: 3.2939...  4.2230 sec/batch\n",
      "Epoch: 2/20...  Training Step: 93...  Training loss: 3.2240...  4.3251 sec/batch\n",
      "Epoch: 2/20...  Training Step: 94...  Training loss: 3.2617...  4.2430 sec/batch\n",
      "Epoch: 2/20...  Training Step: 95...  Training loss: 3.0934...  5.6060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 96...  Training loss: 2.9085...  5.0176 sec/batch\n",
      "Epoch: 2/20...  Training Step: 97...  Training loss: 2.9506...  4.2921 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 98...  Training loss: 2.8997...  4.3711 sec/batch\n",
      "Epoch: 2/20...  Training Step: 99...  Training loss: 2.9078...  5.3308 sec/batch\n",
      "Epoch: 2/20...  Training Step: 100...  Training loss: 2.8783...  4.4992 sec/batch\n",
      "Epoch: 2/20...  Training Step: 101...  Training loss: 2.9949...  4.2670 sec/batch\n",
      "Epoch: 2/20...  Training Step: 102...  Training loss: 2.9454...  5.3498 sec/batch\n",
      "Epoch: 2/20...  Training Step: 103...  Training loss: 2.8641...  6.8018 sec/batch\n",
      "Epoch: 2/20...  Training Step: 104...  Training loss: 2.8849...  6.0063 sec/batch\n",
      "Epoch: 2/20...  Training Step: 105...  Training loss: 2.8936...  5.0536 sec/batch\n",
      "Epoch: 2/20...  Training Step: 106...  Training loss: 2.8305...  4.3681 sec/batch\n",
      "Epoch: 2/20...  Training Step: 107...  Training loss: 2.9103...  4.5883 sec/batch\n",
      "Epoch: 2/20...  Training Step: 108...  Training loss: 2.8376...  4.0319 sec/batch\n",
      "Epoch: 2/20...  Training Step: 109...  Training loss: 2.8700...  3.8547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 110...  Training loss: 2.8259...  3.7897 sec/batch\n",
      "Epoch: 2/20...  Training Step: 111...  Training loss: 2.8214...  4.3151 sec/batch\n",
      "Epoch: 2/20...  Training Step: 112...  Training loss: 2.8912...  5.1787 sec/batch\n",
      "Epoch: 2/20...  Training Step: 113...  Training loss: 2.9182...  5.0886 sec/batch\n",
      "Epoch: 2/20...  Training Step: 114...  Training loss: 2.8623...  5.1497 sec/batch\n",
      "Epoch: 2/20...  Training Step: 115...  Training loss: 2.8404...  4.2120 sec/batch\n",
      "Epoch: 2/20...  Training Step: 116...  Training loss: 2.8502...  5.3838 sec/batch\n",
      "Epoch: 2/20...  Training Step: 117...  Training loss: 2.8269...  5.7241 sec/batch\n",
      "Epoch: 2/20...  Training Step: 118...  Training loss: 2.8705...  4.2590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 119...  Training loss: 2.8631...  4.1199 sec/batch\n",
      "Epoch: 2/20...  Training Step: 120...  Training loss: 2.8264...  4.7083 sec/batch\n",
      "Epoch: 2/20...  Training Step: 121...  Training loss: 2.8844...  4.4452 sec/batch\n",
      "Epoch: 2/20...  Training Step: 122...  Training loss: 2.8469...  4.0779 sec/batch\n",
      "Epoch: 2/20...  Training Step: 123...  Training loss: 2.7514...  3.8617 sec/batch\n",
      "Epoch: 2/20...  Training Step: 124...  Training loss: 2.8670...  3.8417 sec/batch\n",
      "Epoch: 2/20...  Training Step: 125...  Training loss: 2.7748...  3.9138 sec/batch\n",
      "Epoch: 2/20...  Training Step: 126...  Training loss: 2.7974...  3.8948 sec/batch\n",
      "Epoch: 2/20...  Training Step: 127...  Training loss: 2.7385...  4.5172 sec/batch\n",
      "Epoch: 2/20...  Training Step: 128...  Training loss: 2.8337...  4.3381 sec/batch\n",
      "Epoch: 2/20...  Training Step: 129...  Training loss: 2.8289...  5.0096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 130...  Training loss: 2.7784...  4.4662 sec/batch\n",
      "Epoch: 2/20...  Training Step: 131...  Training loss: 2.8327...  4.1860 sec/batch\n",
      "Epoch: 2/20...  Training Step: 132...  Training loss: 2.8478...  5.1377 sec/batch\n",
      "Epoch: 3/20...  Training Step: 133...  Training loss: 2.9135...  4.4372 sec/batch\n",
      "Epoch: 3/20...  Training Step: 134...  Training loss: 2.8374...  4.5332 sec/batch\n",
      "Epoch: 3/20...  Training Step: 135...  Training loss: 2.7481...  4.3541 sec/batch\n",
      "Epoch: 3/20...  Training Step: 136...  Training loss: 2.7459...  4.5112 sec/batch\n",
      "Epoch: 3/20...  Training Step: 137...  Training loss: 2.7449...  4.0028 sec/batch\n",
      "Epoch: 3/20...  Training Step: 138...  Training loss: 2.6762...  3.9798 sec/batch\n",
      "Epoch: 3/20...  Training Step: 139...  Training loss: 2.8210...  4.2100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 140...  Training loss: 2.8622...  4.8615 sec/batch\n",
      "Epoch: 3/20...  Training Step: 141...  Training loss: 2.8072...  4.0209 sec/batch\n",
      "Epoch: 3/20...  Training Step: 142...  Training loss: 2.7712...  4.5232 sec/batch\n",
      "Epoch: 3/20...  Training Step: 143...  Training loss: 2.8122...  6.2304 sec/batch\n",
      "Epoch: 3/20...  Training Step: 144...  Training loss: 2.7280...  4.6303 sec/batch\n",
      "Epoch: 3/20...  Training Step: 145...  Training loss: 2.7663...  4.1970 sec/batch\n",
      "Epoch: 3/20...  Training Step: 146...  Training loss: 2.7994...  4.3261 sec/batch\n",
      "Epoch: 3/20...  Training Step: 147...  Training loss: 2.7486...  4.8074 sec/batch\n",
      "Epoch: 3/20...  Training Step: 148...  Training loss: 2.6747...  5.3388 sec/batch\n",
      "Epoch: 3/20...  Training Step: 149...  Training loss: 2.7609...  4.9435 sec/batch\n",
      "Epoch: 3/20...  Training Step: 150...  Training loss: 2.8051...  4.2350 sec/batch\n",
      "Epoch: 3/20...  Training Step: 151...  Training loss: 2.7385...  4.6443 sec/batch\n",
      "Epoch: 3/20...  Training Step: 152...  Training loss: 2.7160...  4.6153 sec/batch\n",
      "Epoch: 3/20...  Training Step: 153...  Training loss: 2.7091...  3.9998 sec/batch\n",
      "Epoch: 3/20...  Training Step: 154...  Training loss: 2.7079...  3.9728 sec/batch\n",
      "Epoch: 3/20...  Training Step: 155...  Training loss: 2.7350...  4.0169 sec/batch\n",
      "Epoch: 3/20...  Training Step: 156...  Training loss: 2.7108...  4.8484 sec/batch\n",
      "Epoch: 3/20...  Training Step: 157...  Training loss: 2.6602...  3.9828 sec/batch\n",
      "Epoch: 3/20...  Training Step: 158...  Training loss: 2.7374...  4.3711 sec/batch\n",
      "Epoch: 3/20...  Training Step: 159...  Training loss: 2.6459...  4.4181 sec/batch\n",
      "Epoch: 3/20...  Training Step: 160...  Training loss: 2.7286...  4.0439 sec/batch\n",
      "Epoch: 3/20...  Training Step: 161...  Training loss: 2.6706...  3.9858 sec/batch\n",
      "Epoch: 3/20...  Training Step: 162...  Training loss: 2.6540...  3.9738 sec/batch\n",
      "Epoch: 3/20...  Training Step: 163...  Training loss: 2.7093...  3.9998 sec/batch\n",
      "Epoch: 3/20...  Training Step: 164...  Training loss: 2.6331...  3.9728 sec/batch\n",
      "Epoch: 3/20...  Training Step: 165...  Training loss: 2.6353...  3.9528 sec/batch\n",
      "Epoch: 3/20...  Training Step: 166...  Training loss: 2.6265...  4.0769 sec/batch\n",
      "Epoch: 3/20...  Training Step: 167...  Training loss: 2.7158...  4.0008 sec/batch\n",
      "Epoch: 3/20...  Training Step: 168...  Training loss: 2.6881...  3.9948 sec/batch\n",
      "Epoch: 3/20...  Training Step: 169...  Training loss: 2.5887...  4.0769 sec/batch\n",
      "Epoch: 3/20...  Training Step: 170...  Training loss: 2.6127...  4.8865 sec/batch\n",
      "Epoch: 3/20...  Training Step: 171...  Training loss: 2.6237...  4.2550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 172...  Training loss: 2.5538...  4.1499 sec/batch\n",
      "Epoch: 3/20...  Training Step: 173...  Training loss: 2.6577...  3.9038 sec/batch\n",
      "Epoch: 3/20...  Training Step: 174...  Training loss: 2.5471...  4.0529 sec/batch\n",
      "Epoch: 3/20...  Training Step: 175...  Training loss: 2.6201...  3.9818 sec/batch\n",
      "Epoch: 3/20...  Training Step: 176...  Training loss: 2.5368...  3.8948 sec/batch\n",
      "Epoch: 3/20...  Training Step: 177...  Training loss: 2.5761...  4.1790 sec/batch\n",
      "Epoch: 3/20...  Training Step: 178...  Training loss: 2.5876...  3.9068 sec/batch\n",
      "Epoch: 3/20...  Training Step: 179...  Training loss: 2.6393...  3.8627 sec/batch\n",
      "Epoch: 3/20...  Training Step: 180...  Training loss: 2.6172...  4.1980 sec/batch\n",
      "Epoch: 3/20...  Training Step: 181...  Training loss: 2.5563...  3.9343 sec/batch\n",
      "Epoch: 3/20...  Training Step: 182...  Training loss: 2.5671...  4.2750 sec/batch\n",
      "Epoch: 3/20...  Training Step: 183...  Training loss: 2.5548...  4.5733 sec/batch\n",
      "Epoch: 3/20...  Training Step: 184...  Training loss: 2.5978...  6.7698 sec/batch\n",
      "Epoch: 3/20...  Training Step: 185...  Training loss: 2.5933...  5.1296 sec/batch\n",
      "Epoch: 3/20...  Training Step: 186...  Training loss: 2.5501...  5.3148 sec/batch\n",
      "Epoch: 3/20...  Training Step: 187...  Training loss: 2.5998...  4.4141 sec/batch\n",
      "Epoch: 3/20...  Training Step: 188...  Training loss: 2.5687...  4.7914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 189...  Training loss: 2.4757...  4.6133 sec/batch\n",
      "Epoch: 3/20...  Training Step: 190...  Training loss: 2.5645...  4.2870 sec/batch\n",
      "Epoch: 3/20...  Training Step: 191...  Training loss: 2.4894...  4.4031 sec/batch\n",
      "Epoch: 3/20...  Training Step: 192...  Training loss: 2.5092...  4.1249 sec/batch\n",
      "Epoch: 3/20...  Training Step: 193...  Training loss: 2.4662...  4.2991 sec/batch\n",
      "Epoch: 3/20...  Training Step: 194...  Training loss: 2.5696...  4.6153 sec/batch\n",
      "Epoch: 3/20...  Training Step: 195...  Training loss: 2.5452...  4.5282 sec/batch\n",
      "Epoch: 3/20...  Training Step: 196...  Training loss: 2.4807...  4.5212 sec/batch\n",
      "Epoch: 3/20...  Training Step: 197...  Training loss: 2.5437...  5.3538 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 198...  Training loss: 2.5617...  6.7078 sec/batch\n",
      "Epoch: 4/20...  Training Step: 199...  Training loss: 2.6599...  6.4196 sec/batch\n",
      "Epoch: 4/20...  Training Step: 200...  Training loss: 2.5408...  5.5169 sec/batch\n",
      "Epoch: 4/20...  Training Step: 201...  Training loss: 2.4578...  4.1720 sec/batch\n",
      "Epoch: 4/20...  Training Step: 202...  Training loss: 2.4623...  3.8417 sec/batch\n",
      "Epoch: 4/20...  Training Step: 203...  Training loss: 2.4909...  3.8627 sec/batch\n",
      "Epoch: 4/20...  Training Step: 204...  Training loss: 2.3894...  4.4512 sec/batch\n",
      "Epoch: 4/20...  Training Step: 205...  Training loss: 2.5169...  3.9488 sec/batch\n",
      "Epoch: 4/20...  Training Step: 206...  Training loss: 2.5082...  4.3461 sec/batch\n",
      "Epoch: 4/20...  Training Step: 207...  Training loss: 2.5277...  3.8277 sec/batch\n",
      "Epoch: 4/20...  Training Step: 208...  Training loss: 2.4828...  3.8197 sec/batch\n",
      "Epoch: 4/20...  Training Step: 209...  Training loss: 2.5065...  3.7717 sec/batch\n",
      "Epoch: 4/20...  Training Step: 210...  Training loss: 2.4196...  4.8575 sec/batch\n",
      "Epoch: 4/20...  Training Step: 211...  Training loss: 2.4792...  5.6350 sec/batch\n",
      "Epoch: 4/20...  Training Step: 212...  Training loss: 2.5196...  5.4269 sec/batch\n",
      "Epoch: 4/20...  Training Step: 213...  Training loss: 2.4424...  5.5249 sec/batch\n",
      "Epoch: 4/20...  Training Step: 214...  Training loss: 2.4036...  4.2640 sec/batch\n",
      "Epoch: 4/20...  Training Step: 215...  Training loss: 2.4772...  4.6853 sec/batch\n",
      "Epoch: 4/20...  Training Step: 216...  Training loss: 2.5632...  4.6133 sec/batch\n",
      "Epoch: 4/20...  Training Step: 217...  Training loss: 2.4677...  5.1927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 218...  Training loss: 2.4309...  5.0716 sec/batch\n",
      "Epoch: 4/20...  Training Step: 219...  Training loss: 2.4272...  3.9308 sec/batch\n",
      "Epoch: 4/20...  Training Step: 220...  Training loss: 2.4396...  5.0656 sec/batch\n",
      "Epoch: 4/20...  Training Step: 221...  Training loss: 2.4723...  3.9528 sec/batch\n",
      "Epoch: 4/20...  Training Step: 222...  Training loss: 2.4718...  4.1239 sec/batch\n",
      "Epoch: 4/20...  Training Step: 223...  Training loss: 2.4173...  5.4289 sec/batch\n",
      "Epoch: 4/20...  Training Step: 224...  Training loss: 2.4959...  3.9998 sec/batch\n",
      "Epoch: 4/20...  Training Step: 225...  Training loss: 2.4126...  3.8227 sec/batch\n",
      "Epoch: 4/20...  Training Step: 226...  Training loss: 2.4939...  3.7877 sec/batch\n",
      "Epoch: 4/20...  Training Step: 227...  Training loss: 2.4464...  3.7687 sec/batch\n",
      "Epoch: 4/20...  Training Step: 228...  Training loss: 2.4318...  3.8627 sec/batch\n",
      "Epoch: 4/20...  Training Step: 229...  Training loss: 2.4960...  3.8377 sec/batch\n",
      "Epoch: 4/20...  Training Step: 230...  Training loss: 2.4108...  4.5292 sec/batch\n",
      "Epoch: 4/20...  Training Step: 231...  Training loss: 2.4074...  4.8995 sec/batch\n",
      "Epoch: 4/20...  Training Step: 232...  Training loss: 2.4225...  5.3488 sec/batch\n",
      "Epoch: 4/20...  Training Step: 233...  Training loss: 2.5326...  4.2460 sec/batch\n",
      "Epoch: 4/20...  Training Step: 234...  Training loss: 2.4990...  4.8815 sec/batch\n",
      "Epoch: 4/20...  Training Step: 235...  Training loss: 2.4324...  4.7063 sec/batch\n",
      "Epoch: 4/20...  Training Step: 236...  Training loss: 2.4531...  5.4459 sec/batch\n",
      "Epoch: 4/20...  Training Step: 237...  Training loss: 2.4777...  4.3041 sec/batch\n",
      "Epoch: 4/20...  Training Step: 238...  Training loss: 2.4095...  4.2790 sec/batch\n",
      "Epoch: 4/20...  Training Step: 239...  Training loss: 2.4656...  5.5079 sec/batch\n",
      "Epoch: 4/20...  Training Step: 240...  Training loss: 2.4086...  4.6133 sec/batch\n",
      "Epoch: 4/20...  Training Step: 241...  Training loss: 2.4388...  4.8434 sec/batch\n",
      "Epoch: 4/20...  Training Step: 242...  Training loss: 2.3825...  3.9457 sec/batch\n",
      "Epoch: 4/20...  Training Step: 243...  Training loss: 2.4260...  3.9068 sec/batch\n",
      "Epoch: 4/20...  Training Step: 244...  Training loss: 2.4530...  3.8898 sec/batch\n",
      "Epoch: 4/20...  Training Step: 245...  Training loss: 2.4931...  4.9485 sec/batch\n",
      "Epoch: 4/20...  Training Step: 246...  Training loss: 2.4327...  5.4549 sec/batch\n",
      "Epoch: 4/20...  Training Step: 247...  Training loss: 2.4289...  4.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 248...  Training loss: 2.3919...  4.4191 sec/batch\n",
      "Epoch: 4/20...  Training Step: 249...  Training loss: 2.4029...  4.5632 sec/batch\n",
      "Epoch: 4/20...  Training Step: 250...  Training loss: 2.4445...  4.3231 sec/batch\n",
      "Epoch: 4/20...  Training Step: 251...  Training loss: 2.4416...  5.7501 sec/batch\n",
      "Epoch: 4/20...  Training Step: 252...  Training loss: 2.4078...  3.8007 sec/batch\n",
      "Epoch: 4/20...  Training Step: 253...  Training loss: 2.4499...  3.8758 sec/batch\n",
      "Epoch: 4/20...  Training Step: 254...  Training loss: 2.4217...  3.8848 sec/batch\n",
      "Epoch: 4/20...  Training Step: 255...  Training loss: 2.3189...  4.4392 sec/batch\n",
      "Epoch: 4/20...  Training Step: 256...  Training loss: 2.4474...  3.8577 sec/batch\n",
      "Epoch: 4/20...  Training Step: 257...  Training loss: 2.3515...  4.1169 sec/batch\n",
      "Epoch: 4/20...  Training Step: 258...  Training loss: 2.3740...  3.8397 sec/batch\n",
      "Epoch: 4/20...  Training Step: 259...  Training loss: 2.3331...  3.9428 sec/batch\n",
      "Epoch: 4/20...  Training Step: 260...  Training loss: 2.4373...  4.0068 sec/batch\n",
      "Epoch: 4/20...  Training Step: 261...  Training loss: 2.4156...  4.6423 sec/batch\n",
      "Epoch: 4/20...  Training Step: 262...  Training loss: 2.3358...  5.7781 sec/batch\n",
      "Epoch: 4/20...  Training Step: 263...  Training loss: 2.4165...  5.9292 sec/batch\n",
      "Epoch: 4/20...  Training Step: 264...  Training loss: 2.4228...  6.5196 sec/batch\n",
      "Epoch: 5/20...  Training Step: 265...  Training loss: 2.5008...  6.2815 sec/batch\n",
      "Epoch: 5/20...  Training Step: 266...  Training loss: 2.4139...  4.0159 sec/batch\n",
      "Epoch: 5/20...  Training Step: 267...  Training loss: 2.3544...  5.1126 sec/batch\n",
      "Epoch: 5/20...  Training Step: 268...  Training loss: 2.3469...  4.5712 sec/batch\n",
      "Epoch: 5/20...  Training Step: 269...  Training loss: 2.3671...  4.2350 sec/batch\n",
      "Epoch: 5/20...  Training Step: 270...  Training loss: 2.2793...  3.8968 sec/batch\n",
      "Epoch: 5/20...  Training Step: 271...  Training loss: 2.4038...  3.9328 sec/batch\n",
      "Epoch: 5/20...  Training Step: 272...  Training loss: 2.3944...  3.9198 sec/batch\n",
      "Epoch: 5/20...  Training Step: 273...  Training loss: 2.3949...  4.1920 sec/batch\n",
      "Epoch: 5/20...  Training Step: 274...  Training loss: 2.3819...  4.0088 sec/batch\n",
      "Epoch: 5/20...  Training Step: 275...  Training loss: 2.3959...  3.9238 sec/batch\n",
      "Epoch: 5/20...  Training Step: 276...  Training loss: 2.3278...  4.5052 sec/batch\n",
      "Epoch: 5/20...  Training Step: 277...  Training loss: 2.3485...  3.8848 sec/batch\n",
      "Epoch: 5/20...  Training Step: 278...  Training loss: 2.4135...  3.8898 sec/batch\n",
      "Epoch: 5/20...  Training Step: 279...  Training loss: 2.3329...  3.8487 sec/batch\n",
      "Epoch: 5/20...  Training Step: 280...  Training loss: 2.3042...  3.8577 sec/batch\n",
      "Epoch: 5/20...  Training Step: 281...  Training loss: 2.3908...  3.8617 sec/batch\n",
      "Epoch: 5/20...  Training Step: 282...  Training loss: 2.4624...  3.9258 sec/batch\n",
      "Epoch: 5/20...  Training Step: 283...  Training loss: 2.3765...  3.9228 sec/batch\n",
      "Epoch: 5/20...  Training Step: 284...  Training loss: 2.3518...  3.8527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 285...  Training loss: 2.3211...  3.8367 sec/batch\n",
      "Epoch: 5/20...  Training Step: 286...  Training loss: 2.3253...  3.9288 sec/batch\n",
      "Epoch: 5/20...  Training Step: 287...  Training loss: 2.3849...  3.9798 sec/batch\n",
      "Epoch: 5/20...  Training Step: 288...  Training loss: 2.3693...  3.8587 sec/batch\n",
      "Epoch: 5/20...  Training Step: 289...  Training loss: 2.3259...  3.8287 sec/batch\n",
      "Epoch: 5/20...  Training Step: 290...  Training loss: 2.3984...  3.8567 sec/batch\n",
      "Epoch: 5/20...  Training Step: 291...  Training loss: 2.3013...  4.4812 sec/batch\n",
      "Epoch: 5/20...  Training Step: 292...  Training loss: 2.4064...  3.8998 sec/batch\n",
      "Epoch: 5/20...  Training Step: 293...  Training loss: 2.3473...  3.8487 sec/batch\n",
      "Epoch: 5/20...  Training Step: 294...  Training loss: 2.3084...  3.8838 sec/batch\n",
      "Epoch: 5/20...  Training Step: 295...  Training loss: 2.3793...  3.8497 sec/batch\n",
      "Epoch: 5/20...  Training Step: 296...  Training loss: 2.3216...  3.8277 sec/batch\n",
      "Epoch: 5/20...  Training Step: 297...  Training loss: 2.3355...  3.8828 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 298...  Training loss: 2.3354...  3.9958 sec/batch\n",
      "Epoch: 5/20...  Training Step: 299...  Training loss: 2.4190...  3.9228 sec/batch\n",
      "Epoch: 5/20...  Training Step: 300...  Training loss: 2.3980...  3.8687 sec/batch\n",
      "Epoch: 5/20...  Training Step: 301...  Training loss: 2.2981...  3.8217 sec/batch\n",
      "Epoch: 5/20...  Training Step: 302...  Training loss: 2.3363...  3.8207 sec/batch\n",
      "Epoch: 5/20...  Training Step: 303...  Training loss: 2.3618...  4.9955 sec/batch\n",
      "Epoch: 5/20...  Training Step: 304...  Training loss: 2.2643...  4.3891 sec/batch\n",
      "Epoch: 5/20...  Training Step: 305...  Training loss: 2.3617...  4.7954 sec/batch\n",
      "Epoch: 5/20...  Training Step: 306...  Training loss: 2.3104...  4.9725 sec/batch\n",
      "Epoch: 5/20...  Training Step: 307...  Training loss: 2.3360...  4.4562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 308...  Training loss: 2.2575...  3.9038 sec/batch\n",
      "Epoch: 5/20...  Training Step: 309...  Training loss: 2.3181...  4.0829 sec/batch\n",
      "Epoch: 5/20...  Training Step: 310...  Training loss: 2.3326...  4.0519 sec/batch\n",
      "Epoch: 5/20...  Training Step: 311...  Training loss: 2.3801...  4.1389 sec/batch\n",
      "Epoch: 5/20...  Training Step: 312...  Training loss: 2.3328...  4.0759 sec/batch\n",
      "Epoch: 5/20...  Training Step: 313...  Training loss: 2.3217...  4.0038 sec/batch\n",
      "Epoch: 5/20...  Training Step: 314...  Training loss: 2.3106...  3.9988 sec/batch\n",
      "Epoch: 5/20...  Training Step: 315...  Training loss: 2.2992...  4.1129 sec/batch\n",
      "Epoch: 5/20...  Training Step: 316...  Training loss: 2.3432...  4.4251 sec/batch\n",
      "Epoch: 5/20...  Training Step: 317...  Training loss: 2.3483...  4.2260 sec/batch\n",
      "Epoch: 5/20...  Training Step: 318...  Training loss: 2.3160...  4.2340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 319...  Training loss: 2.3448...  3.8768 sec/batch\n",
      "Epoch: 5/20...  Training Step: 320...  Training loss: 2.3312...  4.3141 sec/batch\n",
      "Epoch: 5/20...  Training Step: 321...  Training loss: 2.2293...  4.3301 sec/batch\n",
      "Epoch: 5/20...  Training Step: 322...  Training loss: 2.3389...  3.9238 sec/batch\n",
      "Epoch: 5/20...  Training Step: 323...  Training loss: 2.2577...  4.1990 sec/batch\n",
      "Epoch: 5/20...  Training Step: 324...  Training loss: 2.2842...  4.1670 sec/batch\n",
      "Epoch: 5/20...  Training Step: 325...  Training loss: 2.2372...  3.9368 sec/batch\n",
      "Epoch: 5/20...  Training Step: 326...  Training loss: 2.3507...  3.8547 sec/batch\n",
      "Epoch: 5/20...  Training Step: 327...  Training loss: 2.3267...  3.8367 sec/batch\n",
      "Epoch: 5/20...  Training Step: 328...  Training loss: 2.2381...  3.8027 sec/batch\n",
      "Epoch: 5/20...  Training Step: 329...  Training loss: 2.3231...  3.8527 sec/batch\n",
      "Epoch: 5/20...  Training Step: 330...  Training loss: 2.3137...  4.0349 sec/batch\n",
      "Epoch: 6/20...  Training Step: 331...  Training loss: 2.4227...  3.8217 sec/batch\n",
      "Epoch: 6/20...  Training Step: 332...  Training loss: 2.3336...  3.8758 sec/batch\n",
      "Epoch: 6/20...  Training Step: 333...  Training loss: 2.2548...  3.7999 sec/batch\n",
      "Epoch: 6/20...  Training Step: 334...  Training loss: 2.2376...  3.8033 sec/batch\n",
      "Epoch: 6/20...  Training Step: 335...  Training loss: 2.2864...  4.4161 sec/batch\n",
      "Epoch: 6/20...  Training Step: 336...  Training loss: 2.2056...  3.8647 sec/batch\n",
      "Epoch: 6/20...  Training Step: 337...  Training loss: 2.3149...  3.8557 sec/batch\n",
      "Epoch: 6/20...  Training Step: 338...  Training loss: 2.3188...  3.8197 sec/batch\n",
      "Epoch: 6/20...  Training Step: 339...  Training loss: 2.3336...  3.8057 sec/batch\n",
      "Epoch: 6/20...  Training Step: 340...  Training loss: 2.2734...  3.8057 sec/batch\n",
      "Epoch: 6/20...  Training Step: 341...  Training loss: 2.2949...  4.1660 sec/batch\n",
      "Epoch: 6/20...  Training Step: 342...  Training loss: 2.2283...  3.8417 sec/batch\n",
      "Epoch: 6/20...  Training Step: 343...  Training loss: 2.2819...  3.8267 sec/batch\n",
      "Epoch: 6/20...  Training Step: 344...  Training loss: 2.3105...  3.8427 sec/batch\n",
      "Epoch: 6/20...  Training Step: 345...  Training loss: 2.2418...  3.8928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 346...  Training loss: 2.1985...  3.8707 sec/batch\n",
      "Epoch: 6/20...  Training Step: 347...  Training loss: 2.2981...  3.8407 sec/batch\n",
      "Epoch: 6/20...  Training Step: 348...  Training loss: 2.3707...  3.7987 sec/batch\n",
      "Epoch: 6/20...  Training Step: 349...  Training loss: 2.2923...  3.8197 sec/batch\n",
      "Epoch: 6/20...  Training Step: 350...  Training loss: 2.2471...  3.8057 sec/batch\n",
      "Epoch: 6/20...  Training Step: 351...  Training loss: 2.2339...  4.5285 sec/batch\n",
      "Epoch: 6/20...  Training Step: 352...  Training loss: 2.2364...  3.8185 sec/batch\n",
      "Epoch: 6/20...  Training Step: 353...  Training loss: 2.2837...  3.8427 sec/batch\n",
      "Epoch: 6/20...  Training Step: 354...  Training loss: 2.2847...  3.7977 sec/batch\n",
      "Epoch: 6/20...  Training Step: 355...  Training loss: 2.2213...  3.8217 sec/batch\n",
      "Epoch: 6/20...  Training Step: 356...  Training loss: 2.3127...  3.8127 sec/batch\n",
      "Epoch: 6/20...  Training Step: 357...  Training loss: 2.2307...  3.9048 sec/batch\n",
      "Epoch: 6/20...  Training Step: 358...  Training loss: 2.3238...  4.6733 sec/batch\n",
      "Epoch: 6/20...  Training Step: 359...  Training loss: 2.2592...  4.0819 sec/batch\n",
      "Epoch: 6/20...  Training Step: 360...  Training loss: 2.2411...  4.0669 sec/batch\n",
      "Epoch: 6/20...  Training Step: 361...  Training loss: 2.2721...  5.1146 sec/batch\n",
      "Epoch: 6/20...  Training Step: 362...  Training loss: 2.2429...  4.1820 sec/batch\n",
      "Epoch: 6/20...  Training Step: 363...  Training loss: 2.2571...  3.9748 sec/batch\n",
      "Epoch: 6/20...  Training Step: 364...  Training loss: 2.2478...  4.1239 sec/batch\n",
      "Epoch: 6/20...  Training Step: 365...  Training loss: 2.3292...  4.6283 sec/batch\n",
      "Epoch: 6/20...  Training Step: 366...  Training loss: 2.3205...  4.0369 sec/batch\n",
      "Epoch: 6/20...  Training Step: 367...  Training loss: 2.2221...  3.9058 sec/batch\n",
      "Epoch: 6/20...  Training Step: 368...  Training loss: 2.2534...  3.8938 sec/batch\n",
      "Epoch: 6/20...  Training Step: 369...  Training loss: 2.2849...  3.8687 sec/batch\n",
      "Epoch: 6/20...  Training Step: 370...  Training loss: 2.1759...  3.8587 sec/batch\n",
      "Epoch: 6/20...  Training Step: 371...  Training loss: 2.2916...  3.8137 sec/batch\n",
      "Epoch: 6/20...  Training Step: 372...  Training loss: 2.2312...  3.8547 sec/batch\n",
      "Epoch: 6/20...  Training Step: 373...  Training loss: 2.2545...  3.8738 sec/batch\n",
      "Epoch: 6/20...  Training Step: 374...  Training loss: 2.1838...  3.8277 sec/batch\n",
      "Epoch: 6/20...  Training Step: 375...  Training loss: 2.2318...  3.8367 sec/batch\n",
      "Epoch: 6/20...  Training Step: 376...  Training loss: 2.2584...  3.7937 sec/batch\n",
      "Epoch: 6/20...  Training Step: 377...  Training loss: 2.3187...  3.8677 sec/batch\n",
      "Epoch: 6/20...  Training Step: 378...  Training loss: 2.2719...  3.7997 sec/batch\n",
      "Epoch: 6/20...  Training Step: 379...  Training loss: 2.2556...  3.7957 sec/batch\n",
      "Epoch: 6/20...  Training Step: 380...  Training loss: 2.2130...  4.1329 sec/batch\n",
      "Epoch: 6/20...  Training Step: 381...  Training loss: 2.2207...  4.6133 sec/batch\n",
      "Epoch: 6/20...  Training Step: 382...  Training loss: 2.2737...  3.9608 sec/batch\n",
      "Epoch: 6/20...  Training Step: 383...  Training loss: 2.2713...  4.2270 sec/batch\n",
      "Epoch: 6/20...  Training Step: 384...  Training loss: 2.2397...  4.0879 sec/batch\n",
      "Epoch: 6/20...  Training Step: 385...  Training loss: 2.2540...  3.8397 sec/batch\n",
      "Epoch: 6/20...  Training Step: 386...  Training loss: 2.2567...  3.8718 sec/batch\n",
      "Epoch: 6/20...  Training Step: 387...  Training loss: 2.1550...  3.9288 sec/batch\n",
      "Epoch: 6/20...  Training Step: 388...  Training loss: 2.2727...  3.8748 sec/batch\n",
      "Epoch: 6/20...  Training Step: 389...  Training loss: 2.1850...  3.9158 sec/batch\n",
      "Epoch: 6/20...  Training Step: 390...  Training loss: 2.2007...  4.2190 sec/batch\n",
      "Epoch: 6/20...  Training Step: 391...  Training loss: 2.1584...  3.9698 sec/batch\n",
      "Epoch: 6/20...  Training Step: 392...  Training loss: 2.2668...  4.1680 sec/batch\n",
      "Epoch: 6/20...  Training Step: 393...  Training loss: 2.2642...  4.0329 sec/batch\n",
      "Epoch: 6/20...  Training Step: 394...  Training loss: 2.1827...  3.8237 sec/batch\n",
      "Epoch: 6/20...  Training Step: 395...  Training loss: 2.2607...  4.2260 sec/batch\n",
      "Epoch: 6/20...  Training Step: 396...  Training loss: 2.2572...  4.2580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 397...  Training loss: 2.3433...  3.9088 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 398...  Training loss: 2.2389...  3.8708 sec/batch\n",
      "Epoch: 7/20...  Training Step: 399...  Training loss: 2.1701...  3.9478 sec/batch\n",
      "Epoch: 7/20...  Training Step: 400...  Training loss: 2.1790...  3.9188 sec/batch\n",
      "Epoch: 7/20...  Training Step: 401...  Training loss: 2.2068...  3.6716 sec/batch\n",
      "Epoch: 7/20...  Training Step: 402...  Training loss: 2.1401...  3.8337 sec/batch\n",
      "Epoch: 7/20...  Training Step: 403...  Training loss: 2.2747...  3.8257 sec/batch\n",
      "Epoch: 7/20...  Training Step: 404...  Training loss: 2.2700...  3.8397 sec/batch\n",
      "Epoch: 7/20...  Training Step: 405...  Training loss: 2.2605...  3.8467 sec/batch\n",
      "Epoch: 7/20...  Training Step: 406...  Training loss: 2.2138...  3.8277 sec/batch\n",
      "Epoch: 7/20...  Training Step: 407...  Training loss: 2.2541...  3.8217 sec/batch\n",
      "Epoch: 7/20...  Training Step: 408...  Training loss: 2.1523...  3.8277 sec/batch\n",
      "Epoch: 7/20...  Training Step: 409...  Training loss: 2.2179...  3.8487 sec/batch\n",
      "Epoch: 7/20...  Training Step: 410...  Training loss: 2.2603...  3.8888 sec/batch\n",
      "Epoch: 7/20...  Training Step: 411...  Training loss: 2.2000...  4.4512 sec/batch\n",
      "Epoch: 7/20...  Training Step: 412...  Training loss: 2.1531...  3.8247 sec/batch\n",
      "Epoch: 7/20...  Training Step: 413...  Training loss: 2.2630...  3.9108 sec/batch\n",
      "Epoch: 7/20...  Training Step: 414...  Training loss: 2.3030...  3.8167 sec/batch\n",
      "Epoch: 7/20...  Training Step: 415...  Training loss: 2.2437...  3.8367 sec/batch\n",
      "Epoch: 7/20...  Training Step: 416...  Training loss: 2.1808...  4.0028 sec/batch\n",
      "Epoch: 7/20...  Training Step: 417...  Training loss: 2.1938...  4.2620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 418...  Training loss: 2.2010...  3.8227 sec/batch\n",
      "Epoch: 7/20...  Training Step: 419...  Training loss: 2.2169...  3.8557 sec/batch\n",
      "Epoch: 7/20...  Training Step: 420...  Training loss: 2.2365...  3.8227 sec/batch\n",
      "Epoch: 7/20...  Training Step: 421...  Training loss: 2.1714...  4.4211 sec/batch\n",
      "Epoch: 7/20...  Training Step: 422...  Training loss: 2.2580...  3.9758 sec/batch\n",
      "Epoch: 7/20...  Training Step: 423...  Training loss: 2.1857...  4.0149 sec/batch\n",
      "Epoch: 7/20...  Training Step: 424...  Training loss: 2.2751...  4.4221 sec/batch\n",
      "Epoch: 7/20...  Training Step: 425...  Training loss: 2.2139...  4.8775 sec/batch\n",
      "Epoch: 7/20...  Training Step: 426...  Training loss: 2.1767...  4.1760 sec/batch\n",
      "Epoch: 7/20...  Training Step: 427...  Training loss: 2.2333...  4.3031 sec/batch\n",
      "Epoch: 7/20...  Training Step: 428...  Training loss: 2.2014...  3.9928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 429...  Training loss: 2.1940...  3.9828 sec/batch\n",
      "Epoch: 7/20...  Training Step: 430...  Training loss: 2.2035...  3.9398 sec/batch\n",
      "Epoch: 7/20...  Training Step: 431...  Training loss: 2.2842...  3.9418 sec/batch\n",
      "Epoch: 7/20...  Training Step: 432...  Training loss: 2.2554...  3.9908 sec/batch\n",
      "Epoch: 7/20...  Training Step: 433...  Training loss: 2.1818...  4.0088 sec/batch\n",
      "Epoch: 7/20...  Training Step: 434...  Training loss: 2.2166...  4.0118 sec/batch\n",
      "Epoch: 7/20...  Training Step: 435...  Training loss: 2.2432...  3.9238 sec/batch\n",
      "Epoch: 7/20...  Training Step: 436...  Training loss: 2.1113...  3.9428 sec/batch\n",
      "Epoch: 7/20...  Training Step: 437...  Training loss: 2.2186...  4.0629 sec/batch\n",
      "Epoch: 7/20...  Training Step: 438...  Training loss: 2.1876...  4.0299 sec/batch\n",
      "Epoch: 7/20...  Training Step: 439...  Training loss: 2.1985...  3.9448 sec/batch\n",
      "Epoch: 7/20...  Training Step: 440...  Training loss: 2.1135...  5.4929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 441...  Training loss: 2.1846...  4.6093 sec/batch\n",
      "Epoch: 7/20...  Training Step: 442...  Training loss: 2.2270...  4.1910 sec/batch\n",
      "Epoch: 7/20...  Training Step: 443...  Training loss: 2.2674...  4.3571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 444...  Training loss: 2.2106...  4.5983 sec/batch\n",
      "Epoch: 7/20...  Training Step: 445...  Training loss: 2.2008...  5.1687 sec/batch\n",
      "Epoch: 7/20...  Training Step: 446...  Training loss: 2.1660...  4.9925 sec/batch\n",
      "Epoch: 7/20...  Training Step: 447...  Training loss: 2.1626...  6.1013 sec/batch\n",
      "Epoch: 7/20...  Training Step: 448...  Training loss: 2.2339...  5.4899 sec/batch\n",
      "Epoch: 7/20...  Training Step: 449...  Training loss: 2.2164...  4.9875 sec/batch\n",
      "Epoch: 7/20...  Training Step: 450...  Training loss: 2.1978...  5.2838 sec/batch\n",
      "Epoch: 7/20...  Training Step: 451...  Training loss: 2.2186...  4.8284 sec/batch\n",
      "Epoch: 7/20...  Training Step: 452...  Training loss: 2.2209...  4.9045 sec/batch\n",
      "Epoch: 7/20...  Training Step: 453...  Training loss: 2.1232...  5.0986 sec/batch\n",
      "Epoch: 7/20...  Training Step: 454...  Training loss: 2.2111...  5.0176 sec/batch\n",
      "Epoch: 7/20...  Training Step: 455...  Training loss: 2.1337...  4.5012 sec/batch\n",
      "Epoch: 7/20...  Training Step: 456...  Training loss: 2.1524...  4.2540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 457...  Training loss: 2.1184...  4.2750 sec/batch\n",
      "Epoch: 7/20...  Training Step: 458...  Training loss: 2.2152...  4.2330 sec/batch\n",
      "Epoch: 7/20...  Training Step: 459...  Training loss: 2.2223...  5.1236 sec/batch\n",
      "Epoch: 7/20...  Training Step: 460...  Training loss: 2.1313...  4.5172 sec/batch\n",
      "Epoch: 7/20...  Training Step: 461...  Training loss: 2.2184...  4.2200 sec/batch\n",
      "Epoch: 7/20...  Training Step: 462...  Training loss: 2.1984...  4.8474 sec/batch\n",
      "Epoch: 8/20...  Training Step: 463...  Training loss: 2.2976...  4.2800 sec/batch\n",
      "Epoch: 8/20...  Training Step: 464...  Training loss: 2.2101...  4.2170 sec/batch\n",
      "Epoch: 8/20...  Training Step: 465...  Training loss: 2.1089...  4.8985 sec/batch\n",
      "Epoch: 8/20...  Training Step: 466...  Training loss: 2.1315...  4.3681 sec/batch\n",
      "Epoch: 8/20...  Training Step: 467...  Training loss: 2.1757...  4.8354 sec/batch\n",
      "Epoch: 8/20...  Training Step: 468...  Training loss: 2.1016...  4.7133 sec/batch\n",
      "Epoch: 8/20...  Training Step: 469...  Training loss: 2.2110...  4.2250 sec/batch\n",
      "Epoch: 8/20...  Training Step: 470...  Training loss: 2.2260...  5.6050 sec/batch\n",
      "Epoch: 8/20...  Training Step: 471...  Training loss: 2.2320...  4.4922 sec/batch\n",
      "Epoch: 8/20...  Training Step: 472...  Training loss: 2.1830...  4.5983 sec/batch\n",
      "Epoch: 8/20...  Training Step: 473...  Training loss: 2.2017...  4.5733 sec/batch\n",
      "Epoch: 8/20...  Training Step: 474...  Training loss: 2.1146...  5.7291 sec/batch\n",
      "Epoch: 8/20...  Training Step: 475...  Training loss: 2.1746...  4.3651 sec/batch\n",
      "Epoch: 8/20...  Training Step: 476...  Training loss: 2.2199...  5.0026 sec/batch\n",
      "Epoch: 8/20...  Training Step: 477...  Training loss: 2.1518...  6.8769 sec/batch\n",
      "Epoch: 8/20...  Training Step: 478...  Training loss: 2.1201...  6.7798 sec/batch\n",
      "Epoch: 8/20...  Training Step: 479...  Training loss: 2.2189...  6.6067 sec/batch\n",
      "Epoch: 8/20...  Training Step: 480...  Training loss: 2.2645...  4.7204 sec/batch\n",
      "Epoch: 8/20...  Training Step: 481...  Training loss: 2.2096...  4.6313 sec/batch\n",
      "Epoch: 8/20...  Training Step: 482...  Training loss: 2.1540...  4.3511 sec/batch\n",
      "Epoch: 8/20...  Training Step: 483...  Training loss: 2.1390...  4.6743 sec/batch\n",
      "Epoch: 8/20...  Training Step: 484...  Training loss: 2.1603...  4.4772 sec/batch\n",
      "Epoch: 8/20...  Training Step: 485...  Training loss: 2.1798...  4.2300 sec/batch\n",
      "Epoch: 8/20...  Training Step: 486...  Training loss: 2.1742...  4.2650 sec/batch\n",
      "Epoch: 8/20...  Training Step: 487...  Training loss: 2.1386...  5.1507 sec/batch\n",
      "Epoch: 8/20...  Training Step: 488...  Training loss: 2.2189...  4.5883 sec/batch\n",
      "Epoch: 8/20...  Training Step: 489...  Training loss: 2.1531...  5.6920 sec/batch\n",
      "Epoch: 8/20...  Training Step: 490...  Training loss: 2.2354...  7.1471 sec/batch\n",
      "Epoch: 8/20...  Training Step: 491...  Training loss: 2.1741...  5.9312 sec/batch\n",
      "Epoch: 8/20...  Training Step: 492...  Training loss: 2.1365...  5.2437 sec/batch\n",
      "Epoch: 8/20...  Training Step: 493...  Training loss: 2.1885...  5.5269 sec/batch\n",
      "Epoch: 8/20...  Training Step: 494...  Training loss: 2.1589...  6.6237 sec/batch\n",
      "Epoch: 8/20...  Training Step: 495...  Training loss: 2.1556...  7.2562 sec/batch\n",
      "Epoch: 8/20...  Training Step: 496...  Training loss: 2.1439...  4.7954 sec/batch\n",
      "Epoch: 8/20...  Training Step: 497...  Training loss: 2.2486...  5.5890 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 498...  Training loss: 2.2129...  4.7524 sec/batch\n",
      "Epoch: 8/20...  Training Step: 499...  Training loss: 2.1366...  7.2191 sec/batch\n",
      "Epoch: 8/20...  Training Step: 500...  Training loss: 2.1761...  5.3668 sec/batch\n",
      "Epoch: 8/20...  Training Step: 501...  Training loss: 2.1885...  4.2160 sec/batch\n",
      "Epoch: 8/20...  Training Step: 502...  Training loss: 2.0759...  4.3031 sec/batch\n",
      "Epoch: 8/20...  Training Step: 503...  Training loss: 2.1686...  4.2850 sec/batch\n",
      "Epoch: 8/20...  Training Step: 504...  Training loss: 2.1545...  5.0836 sec/batch\n",
      "Epoch: 8/20...  Training Step: 505...  Training loss: 2.1622...  4.1960 sec/batch\n",
      "Epoch: 8/20...  Training Step: 506...  Training loss: 2.0882...  5.9322 sec/batch\n",
      "Epoch: 8/20...  Training Step: 507...  Training loss: 2.1456...  5.9062 sec/batch\n",
      "Epoch: 8/20...  Training Step: 508...  Training loss: 2.1959...  5.5059 sec/batch\n",
      "Epoch: 8/20...  Training Step: 509...  Training loss: 2.2184...  4.2780 sec/batch\n",
      "Epoch: 8/20...  Training Step: 510...  Training loss: 2.1697...  4.2650 sec/batch\n",
      "Epoch: 8/20...  Training Step: 511...  Training loss: 2.1796...  4.1910 sec/batch\n",
      "Epoch: 8/20...  Training Step: 512...  Training loss: 2.1299...  4.8114 sec/batch\n",
      "Epoch: 8/20...  Training Step: 513...  Training loss: 2.1412...  3.9468 sec/batch\n",
      "Epoch: 8/20...  Training Step: 514...  Training loss: 2.1915...  4.1830 sec/batch\n",
      "Epoch: 8/20...  Training Step: 515...  Training loss: 2.1881...  4.9115 sec/batch\n",
      "Epoch: 8/20...  Training Step: 516...  Training loss: 2.1543...  4.4482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 517...  Training loss: 2.1731...  5.2267 sec/batch\n",
      "Epoch: 8/20...  Training Step: 518...  Training loss: 2.1896...  5.2597 sec/batch\n",
      "Epoch: 8/20...  Training Step: 519...  Training loss: 2.0764...  5.6318 sec/batch\n",
      "Epoch: 8/20...  Training Step: 520...  Training loss: 2.1898...  6.1063 sec/batch\n",
      "Epoch: 8/20...  Training Step: 521...  Training loss: 2.1072...  6.0213 sec/batch\n",
      "Epoch: 8/20...  Training Step: 522...  Training loss: 2.1230...  5.6370 sec/batch\n",
      "Epoch: 8/20...  Training Step: 523...  Training loss: 2.0896...  5.1507 sec/batch\n",
      "Epoch: 8/20...  Training Step: 524...  Training loss: 2.1943...  4.5322 sec/batch\n",
      "Epoch: 8/20...  Training Step: 525...  Training loss: 2.1844...  4.9195 sec/batch\n",
      "Epoch: 8/20...  Training Step: 526...  Training loss: 2.0899...  4.8224 sec/batch\n",
      "Epoch: 8/20...  Training Step: 527...  Training loss: 2.1722...  4.5552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 528...  Training loss: 2.1557...  4.5793 sec/batch\n",
      "Epoch: 9/20...  Training Step: 529...  Training loss: 2.2476...  5.1517 sec/batch\n",
      "Epoch: 9/20...  Training Step: 530...  Training loss: 2.1771...  6.0393 sec/batch\n",
      "Epoch: 9/20...  Training Step: 531...  Training loss: 2.0779...  5.7511 sec/batch\n",
      "Epoch: 9/20...  Training Step: 532...  Training loss: 2.0792...  5.8321 sec/batch\n",
      "Epoch: 9/20...  Training Step: 533...  Training loss: 2.1186...  4.9015 sec/batch\n",
      "Epoch: 9/20...  Training Step: 534...  Training loss: 2.0796...  5.5700 sec/batch\n",
      "Epoch: 9/20...  Training Step: 535...  Training loss: 2.1677...  4.9865 sec/batch\n",
      "Epoch: 9/20...  Training Step: 536...  Training loss: 2.1996...  4.3571 sec/batch\n",
      "Epoch: 9/20...  Training Step: 537...  Training loss: 2.1858...  4.2020 sec/batch\n",
      "Epoch: 9/20...  Training Step: 538...  Training loss: 2.1370...  5.8091 sec/batch\n",
      "Epoch: 9/20...  Training Step: 539...  Training loss: 2.1730...  5.2687 sec/batch\n",
      "Epoch: 9/20...  Training Step: 540...  Training loss: 2.0775...  5.3468 sec/batch\n",
      "Epoch: 9/20...  Training Step: 541...  Training loss: 2.1408...  6.2955 sec/batch\n",
      "Epoch: 9/20...  Training Step: 542...  Training loss: 2.1680...  5.9702 sec/batch\n",
      "Epoch: 9/20...  Training Step: 543...  Training loss: 2.1090...  4.1039 sec/batch\n",
      "Epoch: 9/20...  Training Step: 544...  Training loss: 2.0802...  4.8605 sec/batch\n",
      "Epoch: 9/20...  Training Step: 545...  Training loss: 2.1883...  4.0429 sec/batch\n",
      "Epoch: 9/20...  Training Step: 546...  Training loss: 2.2374...  4.4051 sec/batch\n",
      "Epoch: 9/20...  Training Step: 547...  Training loss: 2.1668...  4.8565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 548...  Training loss: 2.1200...  4.4061 sec/batch\n",
      "Epoch: 9/20...  Training Step: 549...  Training loss: 2.0920...  4.3391 sec/batch\n",
      "Epoch: 9/20...  Training Step: 550...  Training loss: 2.1029...  5.2297 sec/batch\n",
      "Epoch: 9/20...  Training Step: 551...  Training loss: 2.1634...  4.3072 sec/batch\n",
      "Epoch: 9/20...  Training Step: 552...  Training loss: 2.1377...  3.8787 sec/batch\n",
      "Epoch: 9/20...  Training Step: 553...  Training loss: 2.0951...  3.9188 sec/batch\n",
      "Epoch: 9/20...  Training Step: 554...  Training loss: 2.2051...  4.0119 sec/batch\n",
      "Epoch: 9/20...  Training Step: 555...  Training loss: 2.1176...  4.0339 sec/batch\n",
      "Epoch: 9/20...  Training Step: 556...  Training loss: 2.2068...  5.6370 sec/batch\n",
      "Epoch: 9/20...  Training Step: 557...  Training loss: 2.1328...  5.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 558...  Training loss: 2.0939...  4.3651 sec/batch\n",
      "Epoch: 9/20...  Training Step: 559...  Training loss: 2.1604...  4.2730 sec/batch\n",
      "Epoch: 9/20...  Training Step: 560...  Training loss: 2.1347...  4.3771 sec/batch\n",
      "Epoch: 9/20...  Training Step: 561...  Training loss: 2.1173...  4.9325 sec/batch\n",
      "Epoch: 9/20...  Training Step: 562...  Training loss: 2.1145...  5.0596 sec/batch\n",
      "Epoch: 9/20...  Training Step: 563...  Training loss: 2.1934...  4.7534 sec/batch\n",
      "Epoch: 9/20...  Training Step: 564...  Training loss: 2.1888...  5.3678 sec/batch\n",
      "Epoch: 9/20...  Training Step: 565...  Training loss: 2.1051...  4.2050 sec/batch\n",
      "Epoch: 9/20...  Training Step: 566...  Training loss: 2.1371...  4.0239 sec/batch\n",
      "Epoch: 9/20...  Training Step: 567...  Training loss: 2.1760...  4.3391 sec/batch\n",
      "Epoch: 9/20...  Training Step: 568...  Training loss: 2.0512...  3.9778 sec/batch\n",
      "Epoch: 9/20...  Training Step: 569...  Training loss: 2.1398...  3.9588 sec/batch\n",
      "Epoch: 9/20...  Training Step: 570...  Training loss: 2.1363...  4.7164 sec/batch\n",
      "Epoch: 9/20...  Training Step: 571...  Training loss: 2.1290...  5.2687 sec/batch\n",
      "Epoch: 9/20...  Training Step: 572...  Training loss: 2.0437...  5.1657 sec/batch\n",
      "Epoch: 9/20...  Training Step: 573...  Training loss: 2.1118...  4.7584 sec/batch\n",
      "Epoch: 9/20...  Training Step: 574...  Training loss: 2.1377...  5.0646 sec/batch\n",
      "Epoch: 9/20...  Training Step: 575...  Training loss: 2.2001...  4.9485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 576...  Training loss: 2.1364...  4.0969 sec/batch\n",
      "Epoch: 9/20...  Training Step: 577...  Training loss: 2.1611...  4.2040 sec/batch\n",
      "Epoch: 9/20...  Training Step: 578...  Training loss: 2.1086...  3.9268 sec/batch\n",
      "Epoch: 9/20...  Training Step: 579...  Training loss: 2.1184...  4.7033 sec/batch\n",
      "Epoch: 9/20...  Training Step: 580...  Training loss: 2.1612...  3.9238 sec/batch\n",
      "Epoch: 9/20...  Training Step: 581...  Training loss: 2.1462...  5.2898 sec/batch\n",
      "Epoch: 9/20...  Training Step: 582...  Training loss: 2.1360...  3.9048 sec/batch\n",
      "Epoch: 9/20...  Training Step: 583...  Training loss: 2.1282...  3.9478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 584...  Training loss: 2.1649...  3.9018 sec/batch\n",
      "Epoch: 9/20...  Training Step: 585...  Training loss: 2.0578...  3.9193 sec/batch\n",
      "Epoch: 9/20...  Training Step: 586...  Training loss: 2.1729...  4.1189 sec/batch\n",
      "Epoch: 9/20...  Training Step: 587...  Training loss: 2.0689...  4.5202 sec/batch\n",
      "Epoch: 9/20...  Training Step: 588...  Training loss: 2.0780...  4.7424 sec/batch\n",
      "Epoch: 9/20...  Training Step: 589...  Training loss: 2.0420...  4.5252 sec/batch\n",
      "Epoch: 9/20...  Training Step: 590...  Training loss: 2.1571...  5.2517 sec/batch\n",
      "Epoch: 9/20...  Training Step: 591...  Training loss: 2.1496...  4.1970 sec/batch\n",
      "Epoch: 9/20...  Training Step: 592...  Training loss: 2.0794...  4.3921 sec/batch\n",
      "Epoch: 9/20...  Training Step: 593...  Training loss: 2.1303...  4.2340 sec/batch\n",
      "Epoch: 9/20...  Training Step: 594...  Training loss: 2.1295...  4.1509 sec/batch\n",
      "Epoch: 10/20...  Training Step: 595...  Training loss: 2.2385...  4.0569 sec/batch\n",
      "Epoch: 10/20...  Training Step: 596...  Training loss: 2.1155...  4.1610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 597...  Training loss: 2.0361...  4.0889 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 598...  Training loss: 2.0591...  4.0179 sec/batch\n",
      "Epoch: 10/20...  Training Step: 599...  Training loss: 2.0868...  4.1990 sec/batch\n",
      "Epoch: 10/20...  Training Step: 600...  Training loss: 2.0341...  4.7304 sec/batch\n",
      "Epoch: 10/20...  Training Step: 601...  Training loss: 2.1401...  4.8184 sec/batch\n",
      "Epoch: 10/20...  Training Step: 602...  Training loss: 2.1613...  3.9148 sec/batch\n",
      "Epoch: 10/20...  Training Step: 603...  Training loss: 2.1719...  3.9988 sec/batch\n",
      "Epoch: 10/20...  Training Step: 604...  Training loss: 2.1146...  3.9708 sec/batch\n",
      "Epoch: 10/20...  Training Step: 605...  Training loss: 2.1256...  3.9228 sec/batch\n",
      "Epoch: 10/20...  Training Step: 606...  Training loss: 2.0414...  4.1399 sec/batch\n",
      "Epoch: 10/20...  Training Step: 607...  Training loss: 2.1105...  5.0176 sec/batch\n",
      "Epoch: 10/20...  Training Step: 608...  Training loss: 2.1391...  4.6113 sec/batch\n",
      "Epoch: 10/20...  Training Step: 609...  Training loss: 2.0822...  3.9768 sec/batch\n",
      "Epoch: 10/20...  Training Step: 610...  Training loss: 2.0456...  3.9818 sec/batch\n",
      "Epoch: 10/20...  Training Step: 611...  Training loss: 2.1449...  4.5522 sec/batch\n",
      "Epoch: 10/20...  Training Step: 612...  Training loss: 2.2077...  3.8858 sec/batch\n",
      "Epoch: 10/20...  Training Step: 613...  Training loss: 2.1555...  3.8768 sec/batch\n",
      "Epoch: 10/20...  Training Step: 614...  Training loss: 2.0706...  4.1950 sec/batch\n",
      "Epoch: 10/20...  Training Step: 615...  Training loss: 2.0677...  4.9005 sec/batch\n",
      "Epoch: 10/20...  Training Step: 616...  Training loss: 2.0873...  4.4652 sec/batch\n",
      "Epoch: 10/20...  Training Step: 617...  Training loss: 2.1182...  4.8024 sec/batch\n",
      "Epoch: 10/20...  Training Step: 618...  Training loss: 2.1016...  4.6703 sec/batch\n",
      "Epoch: 10/20...  Training Step: 619...  Training loss: 2.0611...  4.3161 sec/batch\n",
      "Epoch: 10/20...  Training Step: 620...  Training loss: 2.1699...  4.8675 sec/batch\n",
      "Epoch: 10/20...  Training Step: 621...  Training loss: 2.0886...  4.1710 sec/batch\n",
      "Epoch: 10/20...  Training Step: 622...  Training loss: 2.1569...  5.2888 sec/batch\n",
      "Epoch: 10/20...  Training Step: 623...  Training loss: 2.1128...  4.5492 sec/batch\n",
      "Epoch: 10/20...  Training Step: 624...  Training loss: 2.0557...  4.2290 sec/batch\n",
      "Epoch: 10/20...  Training Step: 625...  Training loss: 2.1187...  5.5489 sec/batch\n",
      "Epoch: 10/20...  Training Step: 626...  Training loss: 2.1031...  5.7281 sec/batch\n",
      "Epoch: 10/20...  Training Step: 627...  Training loss: 2.0992...  3.8397 sec/batch\n",
      "Epoch: 10/20...  Training Step: 628...  Training loss: 2.0724...  4.5142 sec/batch\n",
      "Epoch: 10/20...  Training Step: 629...  Training loss: 2.1881...  3.8567 sec/batch\n",
      "Epoch: 10/20...  Training Step: 630...  Training loss: 2.1470...  3.8547 sec/batch\n",
      "Epoch: 10/20...  Training Step: 631...  Training loss: 2.0851...  3.8287 sec/batch\n",
      "Epoch: 10/20...  Training Step: 632...  Training loss: 2.0967...  3.8177 sec/batch\n",
      "Epoch: 10/20...  Training Step: 633...  Training loss: 2.1323...  3.7937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 634...  Training loss: 2.0011...  3.7897 sec/batch\n",
      "Epoch: 10/20...  Training Step: 635...  Training loss: 2.1185...  3.7967 sec/batch\n",
      "Epoch: 10/20...  Training Step: 636...  Training loss: 2.0973...  3.7827 sec/batch\n",
      "Epoch: 10/20...  Training Step: 637...  Training loss: 2.1060...  3.7977 sec/batch\n",
      "Epoch: 10/20...  Training Step: 638...  Training loss: 2.0208...  3.8167 sec/batch\n",
      "Epoch: 10/20...  Training Step: 639...  Training loss: 2.0705...  4.3401 sec/batch\n",
      "Epoch: 10/20...  Training Step: 640...  Training loss: 2.1188...  4.5462 sec/batch\n",
      "Epoch: 10/20...  Training Step: 641...  Training loss: 2.1506...  4.4872 sec/batch\n",
      "Epoch: 10/20...  Training Step: 642...  Training loss: 2.1065...  4.3301 sec/batch\n",
      "Epoch: 10/20...  Training Step: 643...  Training loss: 2.1106...  4.7284 sec/batch\n",
      "Epoch: 10/20...  Training Step: 644...  Training loss: 2.0762...  4.5002 sec/batch\n",
      "Epoch: 10/20...  Training Step: 645...  Training loss: 2.0648...  4.7023 sec/batch\n",
      "Epoch: 10/20...  Training Step: 646...  Training loss: 2.1340...  4.7174 sec/batch\n",
      "Epoch: 10/20...  Training Step: 647...  Training loss: 2.1219...  4.9465 sec/batch\n",
      "Epoch: 10/20...  Training Step: 648...  Training loss: 2.1146...  4.5632 sec/batch\n",
      "Epoch: 10/20...  Training Step: 649...  Training loss: 2.1038...  4.7684 sec/batch\n",
      "Epoch: 10/20...  Training Step: 650...  Training loss: 2.1257...  4.6813 sec/batch\n",
      "Epoch: 10/20...  Training Step: 651...  Training loss: 2.0223...  4.6393 sec/batch\n",
      "Epoch: 10/20...  Training Step: 652...  Training loss: 2.1278...  4.5042 sec/batch\n",
      "Epoch: 10/20...  Training Step: 653...  Training loss: 2.0416...  4.7264 sec/batch\n",
      "Epoch: 10/20...  Training Step: 654...  Training loss: 2.0555...  4.4712 sec/batch\n",
      "Epoch: 10/20...  Training Step: 655...  Training loss: 2.0147...  4.5843 sec/batch\n",
      "Epoch: 10/20...  Training Step: 656...  Training loss: 2.1334...  4.5823 sec/batch\n",
      "Epoch: 10/20...  Training Step: 657...  Training loss: 2.1303...  4.8895 sec/batch\n",
      "Epoch: 10/20...  Training Step: 658...  Training loss: 2.0453...  4.7784 sec/batch\n",
      "Epoch: 10/20...  Training Step: 659...  Training loss: 2.1098...  4.7204 sec/batch\n",
      "Epoch: 10/20...  Training Step: 660...  Training loss: 2.0879...  4.7234 sec/batch\n",
      "Epoch: 11/20...  Training Step: 661...  Training loss: 2.1890...  4.7526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 662...  Training loss: 2.0910...  4.7791 sec/batch\n",
      "Epoch: 11/20...  Training Step: 663...  Training loss: 2.0202...  4.7574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 664...  Training loss: 2.0220...  4.7454 sec/batch\n",
      "Epoch: 11/20...  Training Step: 665...  Training loss: 2.0584...  4.6063 sec/batch\n",
      "Epoch: 11/20...  Training Step: 666...  Training loss: 2.0183...  4.6883 sec/batch\n",
      "Epoch: 11/20...  Training Step: 667...  Training loss: 2.1126...  4.5382 sec/batch\n",
      "Epoch: 11/20...  Training Step: 668...  Training loss: 2.1429...  5.0256 sec/batch\n",
      "Epoch: 11/20...  Training Step: 669...  Training loss: 2.1391...  4.5923 sec/batch\n",
      "Epoch: 11/20...  Training Step: 670...  Training loss: 2.0838...  4.4942 sec/batch\n",
      "Epoch: 11/20...  Training Step: 671...  Training loss: 2.1097...  4.5923 sec/batch\n",
      "Epoch: 11/20...  Training Step: 672...  Training loss: 2.0340...  4.7474 sec/batch\n",
      "Epoch: 11/20...  Training Step: 673...  Training loss: 2.0723...  5.0136 sec/batch\n",
      "Epoch: 11/20...  Training Step: 674...  Training loss: 2.1119...  4.8204 sec/batch\n",
      "Epoch: 11/20...  Training Step: 675...  Training loss: 2.0673...  4.9205 sec/batch\n",
      "Epoch: 11/20...  Training Step: 676...  Training loss: 2.0068...  4.8675 sec/batch\n",
      "Epoch: 11/20...  Training Step: 677...  Training loss: 2.1307...  4.5742 sec/batch\n",
      "Epoch: 11/20...  Training Step: 678...  Training loss: 2.1794...  4.9285 sec/batch\n",
      "Epoch: 11/20...  Training Step: 679...  Training loss: 2.1163...  4.5292 sec/batch\n",
      "Epoch: 11/20...  Training Step: 680...  Training loss: 2.0733...  3.8037 sec/batch\n",
      "Epoch: 11/20...  Training Step: 681...  Training loss: 2.0412...  3.8017 sec/batch\n",
      "Epoch: 11/20...  Training Step: 682...  Training loss: 2.0565...  3.8057 sec/batch\n",
      "Epoch: 11/20...  Training Step: 683...  Training loss: 2.0816...  3.8307 sec/batch\n",
      "Epoch: 11/20...  Training Step: 684...  Training loss: 2.0830...  3.7817 sec/batch\n",
      "Epoch: 11/20...  Training Step: 685...  Training loss: 2.0435...  3.7867 sec/batch\n",
      "Epoch: 11/20...  Training Step: 686...  Training loss: 2.1340...  4.1209 sec/batch\n",
      "Epoch: 11/20...  Training Step: 687...  Training loss: 2.0530...  5.1577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 688...  Training loss: 2.1420...  5.6440 sec/batch\n",
      "Epoch: 11/20...  Training Step: 689...  Training loss: 2.0774...  4.0339 sec/batch\n",
      "Epoch: 11/20...  Training Step: 690...  Training loss: 2.0422...  3.7857 sec/batch\n",
      "Epoch: 11/20...  Training Step: 691...  Training loss: 2.0748...  3.8017 sec/batch\n",
      "Epoch: 11/20...  Training Step: 692...  Training loss: 2.0720...  4.6503 sec/batch\n",
      "Epoch: 11/20...  Training Step: 693...  Training loss: 2.0753...  3.8227 sec/batch\n",
      "Epoch: 11/20...  Training Step: 694...  Training loss: 2.0683...  3.8177 sec/batch\n",
      "Epoch: 11/20...  Training Step: 695...  Training loss: 2.1277...  3.7657 sec/batch\n",
      "Epoch: 11/20...  Training Step: 696...  Training loss: 2.1122...  4.6313 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 697...  Training loss: 2.0583...  3.7817 sec/batch\n",
      "Epoch: 11/20...  Training Step: 698...  Training loss: 2.0640...  3.7977 sec/batch\n",
      "Epoch: 11/20...  Training Step: 699...  Training loss: 2.1118...  3.8267 sec/batch\n",
      "Epoch: 11/20...  Training Step: 700...  Training loss: 1.9806...  3.7907 sec/batch\n",
      "Epoch: 11/20...  Training Step: 701...  Training loss: 2.0834...  3.7897 sec/batch\n",
      "Epoch: 11/20...  Training Step: 702...  Training loss: 2.0757...  3.8037 sec/batch\n",
      "Epoch: 11/20...  Training Step: 703...  Training loss: 2.0884...  3.7827 sec/batch\n",
      "Epoch: 11/20...  Training Step: 704...  Training loss: 2.0024...  3.7837 sec/batch\n",
      "Epoch: 11/20...  Training Step: 705...  Training loss: 2.0640...  3.7947 sec/batch\n",
      "Epoch: 11/20...  Training Step: 706...  Training loss: 2.0917...  3.7767 sec/batch\n",
      "Epoch: 11/20...  Training Step: 707...  Training loss: 2.1298...  3.8547 sec/batch\n",
      "Epoch: 11/20...  Training Step: 708...  Training loss: 2.0886...  3.9358 sec/batch\n",
      "Epoch: 11/20...  Training Step: 709...  Training loss: 2.0768...  3.7987 sec/batch\n",
      "Epoch: 11/20...  Training Step: 710...  Training loss: 2.0455...  3.8567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 711...  Training loss: 2.0443...  4.7944 sec/batch\n",
      "Epoch: 11/20...  Training Step: 712...  Training loss: 2.1058...  3.8217 sec/batch\n",
      "Epoch: 11/20...  Training Step: 713...  Training loss: 2.0918...  3.9108 sec/batch\n",
      "Epoch: 11/20...  Training Step: 714...  Training loss: 2.0676...  3.8477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 715...  Training loss: 2.0738...  3.8577 sec/batch\n",
      "Epoch: 11/20...  Training Step: 716...  Training loss: 2.0971...  3.8507 sec/batch\n",
      "Epoch: 11/20...  Training Step: 717...  Training loss: 1.9900...  4.6223 sec/batch\n",
      "Epoch: 11/20...  Training Step: 718...  Training loss: 2.1039...  3.9128 sec/batch\n",
      "Epoch: 11/20...  Training Step: 719...  Training loss: 2.0254...  3.8597 sec/batch\n",
      "Epoch: 11/20...  Training Step: 720...  Training loss: 2.0284...  3.8237 sec/batch\n",
      "Epoch: 11/20...  Training Step: 721...  Training loss: 2.0072...  3.7977 sec/batch\n",
      "Epoch: 11/20...  Training Step: 722...  Training loss: 2.1095...  3.8017 sec/batch\n",
      "Epoch: 11/20...  Training Step: 723...  Training loss: 2.0867...  4.0629 sec/batch\n",
      "Epoch: 11/20...  Training Step: 724...  Training loss: 2.0230...  3.9818 sec/batch\n",
      "Epoch: 11/20...  Training Step: 725...  Training loss: 2.0684...  4.4812 sec/batch\n",
      "Epoch: 11/20...  Training Step: 726...  Training loss: 2.0737...  4.6553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 727...  Training loss: 2.1441...  4.6883 sec/batch\n",
      "Epoch: 12/20...  Training Step: 728...  Training loss: 2.0691...  4.4342 sec/batch\n",
      "Epoch: 12/20...  Training Step: 729...  Training loss: 1.9795...  4.5632 sec/batch\n",
      "Epoch: 12/20...  Training Step: 730...  Training loss: 1.9816...  4.5132 sec/batch\n",
      "Epoch: 12/20...  Training Step: 731...  Training loss: 2.0473...  4.4081 sec/batch\n",
      "Epoch: 12/20...  Training Step: 732...  Training loss: 1.9715...  4.8835 sec/batch\n",
      "Epoch: 12/20...  Training Step: 733...  Training loss: 2.0693...  4.4091 sec/batch\n",
      "Epoch: 12/20...  Training Step: 734...  Training loss: 2.1301...  4.6333 sec/batch\n",
      "Epoch: 12/20...  Training Step: 735...  Training loss: 2.1119...  4.6513 sec/batch\n",
      "Epoch: 12/20...  Training Step: 736...  Training loss: 2.0694...  4.5867 sec/batch\n",
      "Epoch: 12/20...  Training Step: 737...  Training loss: 2.0916...  4.8338 sec/batch\n",
      "Epoch: 12/20...  Training Step: 738...  Training loss: 1.9776...  4.6073 sec/batch\n",
      "Epoch: 12/20...  Training Step: 739...  Training loss: 2.0351...  4.9145 sec/batch\n",
      "Epoch: 12/20...  Training Step: 740...  Training loss: 2.0875...  4.6933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 741...  Training loss: 2.0364...  4.7714 sec/batch\n",
      "Epoch: 12/20...  Training Step: 742...  Training loss: 1.9939...  4.4792 sec/batch\n",
      "Epoch: 12/20...  Training Step: 743...  Training loss: 2.0886...  4.5853 sec/batch\n",
      "Epoch: 12/20...  Training Step: 744...  Training loss: 2.1538...  4.9976 sec/batch\n",
      "Epoch: 12/20...  Training Step: 745...  Training loss: 2.1047...  4.6583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 746...  Training loss: 2.0380...  5.0556 sec/batch\n",
      "Epoch: 12/20...  Training Step: 747...  Training loss: 2.0104...  4.5512 sec/batch\n",
      "Epoch: 12/20...  Training Step: 748...  Training loss: 2.0205...  4.5462 sec/batch\n",
      "Epoch: 12/20...  Training Step: 749...  Training loss: 2.0626...  4.6413 sec/batch\n",
      "Epoch: 12/20...  Training Step: 750...  Training loss: 2.0343...  4.7134 sec/batch\n",
      "Epoch: 12/20...  Training Step: 751...  Training loss: 1.9997...  4.6663 sec/batch\n",
      "Epoch: 12/20...  Training Step: 752...  Training loss: 2.0971...  4.8625 sec/batch\n",
      "Epoch: 12/20...  Training Step: 753...  Training loss: 2.0376...  4.9976 sec/batch\n",
      "Epoch: 12/20...  Training Step: 754...  Training loss: 2.1212...  4.6453 sec/batch\n",
      "Epoch: 12/20...  Training Step: 755...  Training loss: 2.0515...  4.6663 sec/batch\n",
      "Epoch: 12/20...  Training Step: 756...  Training loss: 2.0177...  4.4271 sec/batch\n",
      "Epoch: 12/20...  Training Step: 757...  Training loss: 2.0559...  4.6533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 758...  Training loss: 2.0559...  4.8835 sec/batch\n",
      "Epoch: 12/20...  Training Step: 759...  Training loss: 2.0505...  4.6673 sec/batch\n",
      "Epoch: 12/20...  Training Step: 760...  Training loss: 2.0358...  4.7063 sec/batch\n",
      "Epoch: 12/20...  Training Step: 761...  Training loss: 2.0963...  4.5572 sec/batch\n",
      "Epoch: 12/20...  Training Step: 762...  Training loss: 2.0775...  4.6813 sec/batch\n",
      "Epoch: 12/20...  Training Step: 763...  Training loss: 2.0207...  4.7984 sec/batch\n",
      "Epoch: 12/20...  Training Step: 764...  Training loss: 2.0572...  4.6973 sec/batch\n",
      "Epoch: 12/20...  Training Step: 765...  Training loss: 2.0809...  4.6983 sec/batch\n",
      "Epoch: 12/20...  Training Step: 766...  Training loss: 1.9463...  4.7574 sec/batch\n",
      "Epoch: 12/20...  Training Step: 767...  Training loss: 2.0624...  4.9725 sec/batch\n",
      "Epoch: 12/20...  Training Step: 768...  Training loss: 2.0458...  4.6803 sec/batch\n",
      "Epoch: 12/20...  Training Step: 769...  Training loss: 2.0505...  4.8524 sec/batch\n",
      "Epoch: 12/20...  Training Step: 770...  Training loss: 1.9671...  4.6643 sec/batch\n",
      "Epoch: 12/20...  Training Step: 771...  Training loss: 2.0229...  4.8324 sec/batch\n",
      "Epoch: 12/20...  Training Step: 772...  Training loss: 2.0746...  4.5983 sec/batch\n",
      "Epoch: 12/20...  Training Step: 773...  Training loss: 2.1033...  4.5813 sec/batch\n",
      "Epoch: 12/20...  Training Step: 774...  Training loss: 2.0701...  4.8845 sec/batch\n",
      "Epoch: 12/20...  Training Step: 775...  Training loss: 2.0653...  4.5883 sec/batch\n",
      "Epoch: 12/20...  Training Step: 776...  Training loss: 2.0224...  4.6563 sec/batch\n",
      "Epoch: 12/20...  Training Step: 777...  Training loss: 2.0085...  4.6293 sec/batch\n",
      "Epoch: 12/20...  Training Step: 778...  Training loss: 2.0766...  4.6613 sec/batch\n",
      "Epoch: 12/20...  Training Step: 779...  Training loss: 2.0558...  4.8405 sec/batch\n",
      "Epoch: 12/20...  Training Step: 780...  Training loss: 2.0521...  4.5532 sec/batch\n",
      "Epoch: 12/20...  Training Step: 781...  Training loss: 2.0471...  4.8534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 782...  Training loss: 2.0676...  4.6333 sec/batch\n",
      "Epoch: 12/20...  Training Step: 783...  Training loss: 1.9688...  4.6623 sec/batch\n",
      "Epoch: 12/20...  Training Step: 784...  Training loss: 2.0498...  4.7784 sec/batch\n",
      "Epoch: 12/20...  Training Step: 785...  Training loss: 1.9991...  4.5983 sec/batch\n",
      "Epoch: 12/20...  Training Step: 786...  Training loss: 1.9761...  4.6443 sec/batch\n",
      "Epoch: 12/20...  Training Step: 787...  Training loss: 1.9834...  4.5722 sec/batch\n",
      "Epoch: 12/20...  Training Step: 788...  Training loss: 2.0794...  4.6363 sec/batch\n",
      "Epoch: 12/20...  Training Step: 789...  Training loss: 2.0459...  4.8514 sec/batch\n",
      "Epoch: 12/20...  Training Step: 790...  Training loss: 1.9951...  4.7854 sec/batch\n",
      "Epoch: 12/20...  Training Step: 791...  Training loss: 2.0528...  4.8444 sec/batch\n",
      "Epoch: 12/20...  Training Step: 792...  Training loss: 2.0427...  4.6613 sec/batch\n",
      "Epoch: 13/20...  Training Step: 793...  Training loss: 2.1297...  4.6793 sec/batch\n",
      "Epoch: 13/20...  Training Step: 794...  Training loss: 2.0333...  5.2918 sec/batch\n",
      "Epoch: 13/20...  Training Step: 795...  Training loss: 1.9510...  4.7964 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 796...  Training loss: 1.9625...  4.7154 sec/batch\n",
      "Epoch: 13/20...  Training Step: 797...  Training loss: 2.0134...  4.5692 sec/batch\n",
      "Epoch: 13/20...  Training Step: 798...  Training loss: 1.9342...  4.6983 sec/batch\n",
      "Epoch: 13/20...  Training Step: 799...  Training loss: 2.0502...  4.7624 sec/batch\n",
      "Epoch: 13/20...  Training Step: 800...  Training loss: 2.0886...  4.4872 sec/batch\n",
      "Epoch: 13/20...  Training Step: 801...  Training loss: 2.0830...  4.5452 sec/batch\n",
      "Epoch: 13/20...  Training Step: 802...  Training loss: 2.0254...  4.5372 sec/batch\n",
      "Epoch: 13/20...  Training Step: 803...  Training loss: 2.0509...  5.0576 sec/batch\n",
      "Epoch: 13/20...  Training Step: 804...  Training loss: 1.9528...  4.5432 sec/batch\n",
      "Epoch: 13/20...  Training Step: 805...  Training loss: 2.0178...  4.9385 sec/batch\n",
      "Epoch: 13/20...  Training Step: 806...  Training loss: 2.0471...  4.5692 sec/batch\n",
      "Epoch: 13/20...  Training Step: 807...  Training loss: 2.0052...  5.1286 sec/batch\n",
      "Epoch: 13/20...  Training Step: 808...  Training loss: 1.9578...  5.1907 sec/batch\n",
      "Epoch: 13/20...  Training Step: 809...  Training loss: 2.0648...  4.6753 sec/batch\n",
      "Epoch: 13/20...  Training Step: 810...  Training loss: 2.1361...  4.8474 sec/batch\n",
      "Epoch: 13/20...  Training Step: 811...  Training loss: 2.0744...  4.6653 sec/batch\n",
      "Epoch: 13/20...  Training Step: 812...  Training loss: 2.0036...  4.6513 sec/batch\n",
      "Epoch: 13/20...  Training Step: 813...  Training loss: 1.9821...  4.6713 sec/batch\n",
      "Epoch: 13/20...  Training Step: 814...  Training loss: 1.9867...  4.6853 sec/batch\n",
      "Epoch: 13/20...  Training Step: 815...  Training loss: 2.0492...  4.4031 sec/batch\n",
      "Epoch: 13/20...  Training Step: 816...  Training loss: 2.0162...  4.7013 sec/batch\n",
      "Epoch: 13/20...  Training Step: 817...  Training loss: 1.9980...  4.8054 sec/batch\n",
      "Epoch: 13/20...  Training Step: 818...  Training loss: 2.0730...  4.7924 sec/batch\n",
      "Epoch: 13/20...  Training Step: 819...  Training loss: 2.0128...  4.7434 sec/batch\n",
      "Epoch: 13/20...  Training Step: 820...  Training loss: 2.1035...  4.5572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 821...  Training loss: 2.0348...  4.8585 sec/batch\n",
      "Epoch: 13/20...  Training Step: 822...  Training loss: 1.9939...  4.8374 sec/batch\n",
      "Epoch: 13/20...  Training Step: 823...  Training loss: 2.0242...  4.9155 sec/batch\n",
      "Epoch: 13/20...  Training Step: 824...  Training loss: 2.0232...  4.6313 sec/batch\n",
      "Epoch: 13/20...  Training Step: 825...  Training loss: 2.0221...  4.9465 sec/batch\n",
      "Epoch: 13/20...  Training Step: 826...  Training loss: 1.9929...  4.6303 sec/batch\n",
      "Epoch: 13/20...  Training Step: 827...  Training loss: 2.0642...  4.6493 sec/batch\n",
      "Epoch: 13/20...  Training Step: 828...  Training loss: 2.0681...  4.6743 sec/batch\n",
      "Epoch: 13/20...  Training Step: 829...  Training loss: 2.0070...  4.7023 sec/batch\n",
      "Epoch: 13/20...  Training Step: 830...  Training loss: 2.0335...  4.7504 sec/batch\n",
      "Epoch: 13/20...  Training Step: 831...  Training loss: 2.0491...  4.8765 sec/batch\n",
      "Epoch: 13/20...  Training Step: 832...  Training loss: 1.9227...  4.4692 sec/batch\n",
      "Epoch: 13/20...  Training Step: 833...  Training loss: 2.0225...  4.6603 sec/batch\n",
      "Epoch: 13/20...  Training Step: 834...  Training loss: 2.0156...  4.9986 sec/batch\n",
      "Epoch: 13/20...  Training Step: 835...  Training loss: 2.0193...  4.4231 sec/batch\n",
      "Epoch: 13/20...  Training Step: 836...  Training loss: 1.9410...  4.3881 sec/batch\n",
      "Epoch: 13/20...  Training Step: 837...  Training loss: 1.9953...  4.3821 sec/batch\n",
      "Epoch: 13/20...  Training Step: 838...  Training loss: 2.0559...  4.7474 sec/batch\n",
      "Epoch: 13/20...  Training Step: 839...  Training loss: 2.0634...  4.5262 sec/batch\n",
      "Epoch: 13/20...  Training Step: 840...  Training loss: 2.0347...  4.8464 sec/batch\n",
      "Epoch: 13/20...  Training Step: 841...  Training loss: 2.0360...  4.5192 sec/batch\n",
      "Epoch: 13/20...  Training Step: 842...  Training loss: 1.9990...  4.7204 sec/batch\n",
      "Epoch: 13/20...  Training Step: 843...  Training loss: 2.0001...  4.6963 sec/batch\n",
      "Epoch: 13/20...  Training Step: 844...  Training loss: 2.0490...  4.9375 sec/batch\n",
      "Epoch: 13/20...  Training Step: 845...  Training loss: 2.0402...  4.5893 sec/batch\n",
      "Epoch: 13/20...  Training Step: 846...  Training loss: 2.0220...  4.6803 sec/batch\n",
      "Epoch: 13/20...  Training Step: 847...  Training loss: 2.0263...  4.7964 sec/batch\n",
      "Epoch: 13/20...  Training Step: 848...  Training loss: 2.0353...  4.6593 sec/batch\n",
      "Epoch: 13/20...  Training Step: 849...  Training loss: 1.9548...  4.8805 sec/batch\n",
      "Epoch: 13/20...  Training Step: 850...  Training loss: 2.0551...  4.8374 sec/batch\n",
      "Epoch: 13/20...  Training Step: 851...  Training loss: 1.9783...  4.7784 sec/batch\n",
      "Epoch: 13/20...  Training Step: 852...  Training loss: 1.9756...  4.6813 sec/batch\n",
      "Epoch: 13/20...  Training Step: 853...  Training loss: 1.9393...  4.6503 sec/batch\n",
      "Epoch: 13/20...  Training Step: 854...  Training loss: 2.0591...  4.6013 sec/batch\n",
      "Epoch: 13/20...  Training Step: 855...  Training loss: 2.0301...  4.5332 sec/batch\n",
      "Epoch: 13/20...  Training Step: 856...  Training loss: 1.9680...  4.6933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 857...  Training loss: 2.0248...  4.8384 sec/batch\n",
      "Epoch: 13/20...  Training Step: 858...  Training loss: 2.0253...  4.5132 sec/batch\n",
      "Epoch: 14/20...  Training Step: 859...  Training loss: 2.0986...  4.9745 sec/batch\n",
      "Epoch: 14/20...  Training Step: 860...  Training loss: 2.0108...  4.8615 sec/batch\n",
      "Epoch: 14/20...  Training Step: 861...  Training loss: 1.9301...  4.5712 sec/batch\n",
      "Epoch: 14/20...  Training Step: 862...  Training loss: 1.9364...  4.8064 sec/batch\n",
      "Epoch: 14/20...  Training Step: 863...  Training loss: 1.9719...  4.9205 sec/batch\n",
      "Epoch: 14/20...  Training Step: 864...  Training loss: 1.9413...  4.6173 sec/batch\n",
      "Epoch: 14/20...  Training Step: 865...  Training loss: 2.0131...  4.7404 sec/batch\n",
      "Epoch: 14/20...  Training Step: 866...  Training loss: 2.0748...  4.5052 sec/batch\n",
      "Epoch: 14/20...  Training Step: 867...  Training loss: 2.0531...  4.6313 sec/batch\n",
      "Epoch: 14/20...  Training Step: 868...  Training loss: 2.0082...  4.6163 sec/batch\n",
      "Epoch: 14/20...  Training Step: 869...  Training loss: 2.0410...  4.4402 sec/batch\n",
      "Epoch: 14/20...  Training Step: 870...  Training loss: 1.9222...  4.7524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 871...  Training loss: 2.0043...  5.0696 sec/batch\n",
      "Epoch: 14/20...  Training Step: 872...  Training loss: 2.0116...  4.8855 sec/batch\n",
      "Epoch: 14/20...  Training Step: 873...  Training loss: 1.9664...  4.5152 sec/batch\n",
      "Epoch: 14/20...  Training Step: 874...  Training loss: 1.9473...  4.5472 sec/batch\n",
      "Epoch: 14/20...  Training Step: 875...  Training loss: 2.0477...  4.9325 sec/batch\n",
      "Epoch: 14/20...  Training Step: 876...  Training loss: 2.1205...  4.6968 sec/batch\n",
      "Epoch: 14/20...  Training Step: 877...  Training loss: 2.0589...  4.7294 sec/batch\n",
      "Epoch: 14/20...  Training Step: 878...  Training loss: 1.9837...  4.7083 sec/batch\n",
      "Epoch: 14/20...  Training Step: 879...  Training loss: 1.9737...  4.7063 sec/batch\n",
      "Epoch: 14/20...  Training Step: 880...  Training loss: 1.9806...  4.6863 sec/batch\n",
      "Epoch: 14/20...  Training Step: 881...  Training loss: 2.0097...  4.8685 sec/batch\n",
      "Epoch: 14/20...  Training Step: 882...  Training loss: 1.9874...  4.7424 sec/batch\n",
      "Epoch: 14/20...  Training Step: 883...  Training loss: 1.9530...  4.8412 sec/batch\n",
      "Epoch: 14/20...  Training Step: 884...  Training loss: 2.0425...  4.5913 sec/batch\n",
      "Epoch: 14/20...  Training Step: 885...  Training loss: 1.9693...  4.7974 sec/batch\n",
      "Epoch: 14/20...  Training Step: 886...  Training loss: 2.0609...  4.4452 sec/batch\n",
      "Epoch: 14/20...  Training Step: 887...  Training loss: 2.0193...  4.6743 sec/batch\n",
      "Epoch: 14/20...  Training Step: 888...  Training loss: 1.9617...  4.6263 sec/batch\n",
      "Epoch: 14/20...  Training Step: 889...  Training loss: 2.0026...  4.6853 sec/batch\n",
      "Epoch: 14/20...  Training Step: 890...  Training loss: 2.0039...  4.8404 sec/batch\n",
      "Epoch: 14/20...  Training Step: 891...  Training loss: 2.0117...  4.5652 sec/batch\n",
      "Epoch: 14/20...  Training Step: 892...  Training loss: 1.9719...  4.6453 sec/batch\n",
      "Epoch: 14/20...  Training Step: 893...  Training loss: 2.0441...  4.7394 sec/batch\n",
      "Epoch: 14/20...  Training Step: 894...  Training loss: 2.0255...  4.6313 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 895...  Training loss: 1.9788...  4.9905 sec/batch\n",
      "Epoch: 14/20...  Training Step: 896...  Training loss: 2.0086...  4.5983 sec/batch\n",
      "Epoch: 14/20...  Training Step: 897...  Training loss: 2.0292...  4.6773 sec/batch\n",
      "Epoch: 14/20...  Training Step: 898...  Training loss: 1.9051...  4.6983 sec/batch\n",
      "Epoch: 14/20...  Training Step: 899...  Training loss: 2.0059...  4.9325 sec/batch\n",
      "Epoch: 14/20...  Training Step: 900...  Training loss: 1.9845...  4.5733 sec/batch\n",
      "Epoch: 14/20...  Training Step: 901...  Training loss: 2.0028...  4.7023 sec/batch\n",
      "Epoch: 14/20...  Training Step: 902...  Training loss: 1.9399...  4.5282 sec/batch\n",
      "Epoch: 14/20...  Training Step: 903...  Training loss: 1.9812...  4.7123 sec/batch\n",
      "Epoch: 14/20...  Training Step: 904...  Training loss: 2.0203...  5.1847 sec/batch\n",
      "Epoch: 14/20...  Training Step: 905...  Training loss: 2.0285...  4.6603 sec/batch\n",
      "Epoch: 14/20...  Training Step: 906...  Training loss: 2.0170...  5.0346 sec/batch\n",
      "Epoch: 14/20...  Training Step: 907...  Training loss: 2.0256...  4.6803 sec/batch\n",
      "Epoch: 14/20...  Training Step: 908...  Training loss: 1.9828...  4.4762 sec/batch\n",
      "Epoch: 14/20...  Training Step: 909...  Training loss: 1.9755...  4.5632 sec/batch\n",
      "Epoch: 14/20...  Training Step: 910...  Training loss: 2.0354...  4.9235 sec/batch\n",
      "Epoch: 14/20...  Training Step: 911...  Training loss: 2.0155...  4.7103 sec/batch\n",
      "Epoch: 14/20...  Training Step: 912...  Training loss: 1.9969...  4.6233 sec/batch\n",
      "Epoch: 14/20...  Training Step: 913...  Training loss: 1.9940...  4.6473 sec/batch\n",
      "Epoch: 14/20...  Training Step: 914...  Training loss: 2.0397...  4.4702 sec/batch\n",
      "Epoch: 14/20...  Training Step: 915...  Training loss: 1.9128...  4.7224 sec/batch\n",
      "Epoch: 14/20...  Training Step: 916...  Training loss: 2.0184...  4.6603 sec/batch\n",
      "Epoch: 14/20...  Training Step: 917...  Training loss: 1.9471...  4.9575 sec/batch\n",
      "Epoch: 14/20...  Training Step: 918...  Training loss: 1.9627...  4.6923 sec/batch\n",
      "Epoch: 14/20...  Training Step: 919...  Training loss: 1.9333...  4.5252 sec/batch\n",
      "Epoch: 14/20...  Training Step: 920...  Training loss: 2.0210...  4.8444 sec/batch\n",
      "Epoch: 14/20...  Training Step: 921...  Training loss: 2.0191...  4.4722 sec/batch\n",
      "Epoch: 14/20...  Training Step: 922...  Training loss: 1.9330...  4.6573 sec/batch\n",
      "Epoch: 14/20...  Training Step: 923...  Training loss: 2.0049...  4.8504 sec/batch\n",
      "Epoch: 14/20...  Training Step: 924...  Training loss: 1.9808...  4.7964 sec/batch\n",
      "Epoch: 15/20...  Training Step: 925...  Training loss: 2.0671...  4.6823 sec/batch\n",
      "Epoch: 15/20...  Training Step: 926...  Training loss: 1.9816...  4.6823 sec/batch\n",
      "Epoch: 15/20...  Training Step: 927...  Training loss: 1.9000...  4.7984 sec/batch\n",
      "Epoch: 15/20...  Training Step: 928...  Training loss: 1.9174...  4.7013 sec/batch\n",
      "Epoch: 15/20...  Training Step: 929...  Training loss: 1.9555...  4.9635 sec/batch\n",
      "Epoch: 15/20...  Training Step: 930...  Training loss: 1.9156...  4.6013 sec/batch\n",
      "Epoch: 15/20...  Training Step: 931...  Training loss: 1.9825...  4.8975 sec/batch\n",
      "Epoch: 15/20...  Training Step: 932...  Training loss: 2.0661...  4.6753 sec/batch\n",
      "Epoch: 15/20...  Training Step: 933...  Training loss: 2.0433...  4.4482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 934...  Training loss: 1.9893...  4.9685 sec/batch\n",
      "Epoch: 15/20...  Training Step: 935...  Training loss: 2.0041...  5.2868 sec/batch\n",
      "Epoch: 15/20...  Training Step: 936...  Training loss: 1.9281...  4.6513 sec/batch\n",
      "Epoch: 15/20...  Training Step: 937...  Training loss: 1.9718...  4.7824 sec/batch\n",
      "Epoch: 15/20...  Training Step: 938...  Training loss: 2.0056...  4.9725 sec/batch\n",
      "Epoch: 15/20...  Training Step: 939...  Training loss: 1.9493...  4.6713 sec/batch\n",
      "Epoch: 15/20...  Training Step: 940...  Training loss: 1.9323...  4.6983 sec/batch\n",
      "Epoch: 15/20...  Training Step: 941...  Training loss: 2.0205...  4.5322 sec/batch\n",
      "Epoch: 15/20...  Training Step: 942...  Training loss: 2.0863...  4.4442 sec/batch\n",
      "Epoch: 15/20...  Training Step: 943...  Training loss: 2.0302...  4.8214 sec/batch\n",
      "Epoch: 15/20...  Training Step: 944...  Training loss: 1.9682...  4.9295 sec/batch\n",
      "Epoch: 15/20...  Training Step: 945...  Training loss: 1.9430...  4.8925 sec/batch\n",
      "Epoch: 15/20...  Training Step: 946...  Training loss: 1.9474...  5.1487 sec/batch\n",
      "Epoch: 15/20...  Training Step: 947...  Training loss: 2.0122...  5.1627 sec/batch\n",
      "Epoch: 15/20...  Training Step: 948...  Training loss: 1.9840...  4.9925 sec/batch\n",
      "Epoch: 15/20...  Training Step: 949...  Training loss: 1.9275...  4.9375 sec/batch\n",
      "Epoch: 15/20...  Training Step: 950...  Training loss: 2.0217...  5.0646 sec/batch\n",
      "Epoch: 15/20...  Training Step: 951...  Training loss: 1.9623...  5.0446 sec/batch\n",
      "Epoch: 15/20...  Training Step: 952...  Training loss: 2.0397...  4.8084 sec/batch\n",
      "Epoch: 15/20...  Training Step: 953...  Training loss: 1.9873...  4.6903 sec/batch\n",
      "Epoch: 15/20...  Training Step: 954...  Training loss: 1.9379...  4.7274 sec/batch\n",
      "Epoch: 15/20...  Training Step: 955...  Training loss: 1.9851...  4.7314 sec/batch\n",
      "Epoch: 15/20...  Training Step: 956...  Training loss: 1.9803...  4.6683 sec/batch\n",
      "Epoch: 15/20...  Training Step: 957...  Training loss: 1.9974...  4.8294 sec/batch\n",
      "Epoch: 15/20...  Training Step: 958...  Training loss: 1.9516...  4.6263 sec/batch\n",
      "Epoch: 15/20...  Training Step: 959...  Training loss: 2.0336...  4.7854 sec/batch\n",
      "Epoch: 15/20...  Training Step: 960...  Training loss: 1.9891...  4.6823 sec/batch\n",
      "Epoch: 15/20...  Training Step: 961...  Training loss: 1.9550...  4.7204 sec/batch\n",
      "Epoch: 15/20...  Training Step: 962...  Training loss: 1.9687...  4.6683 sec/batch\n",
      "Epoch: 15/20...  Training Step: 963...  Training loss: 1.9995...  4.7884 sec/batch\n",
      "Epoch: 15/20...  Training Step: 964...  Training loss: 1.8826...  4.6823 sec/batch\n",
      "Epoch: 15/20...  Training Step: 965...  Training loss: 1.9770...  4.6533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 966...  Training loss: 1.9745...  4.5482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 967...  Training loss: 1.9800...  4.7364 sec/batch\n",
      "Epoch: 15/20...  Training Step: 968...  Training loss: 1.8962...  5.1336 sec/batch\n",
      "Epoch: 15/20...  Training Step: 969...  Training loss: 1.9562...  4.9625 sec/batch\n",
      "Epoch: 15/20...  Training Step: 970...  Training loss: 1.9945...  4.7614 sec/batch\n",
      "Epoch: 15/20...  Training Step: 971...  Training loss: 2.0041...  4.5442 sec/batch\n",
      "Epoch: 15/20...  Training Step: 972...  Training loss: 1.9955...  4.6733 sec/batch\n",
      "Epoch: 15/20...  Training Step: 973...  Training loss: 2.0083...  4.7594 sec/batch\n",
      "Epoch: 15/20...  Training Step: 974...  Training loss: 1.9634...  4.7874 sec/batch\n",
      "Epoch: 15/20...  Training Step: 975...  Training loss: 1.9578...  4.6563 sec/batch\n",
      "Epoch: 15/20...  Training Step: 976...  Training loss: 2.0191...  4.7534 sec/batch\n",
      "Epoch: 15/20...  Training Step: 977...  Training loss: 1.9862...  5.0686 sec/batch\n",
      "Epoch: 15/20...  Training Step: 978...  Training loss: 1.9819...  4.6163 sec/batch\n",
      "Epoch: 15/20...  Training Step: 979...  Training loss: 1.9825...  4.9365 sec/batch\n",
      "Epoch: 15/20...  Training Step: 980...  Training loss: 2.0016...  4.9645 sec/batch\n",
      "Epoch: 15/20...  Training Step: 981...  Training loss: 1.8927...  4.9795 sec/batch\n",
      "Epoch: 15/20...  Training Step: 982...  Training loss: 1.9871...  4.6803 sec/batch\n",
      "Epoch: 15/20...  Training Step: 983...  Training loss: 1.9406...  4.7524 sec/batch\n",
      "Epoch: 15/20...  Training Step: 984...  Training loss: 1.9230...  4.6883 sec/batch\n",
      "Epoch: 15/20...  Training Step: 985...  Training loss: 1.9084...  4.7484 sec/batch\n",
      "Epoch: 15/20...  Training Step: 986...  Training loss: 2.0064...  4.6833 sec/batch\n",
      "Epoch: 15/20...  Training Step: 987...  Training loss: 1.9865...  4.7414 sec/batch\n",
      "Epoch: 15/20...  Training Step: 988...  Training loss: 1.9239...  4.7093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 989...  Training loss: 1.9742...  4.6733 sec/batch\n",
      "Epoch: 15/20...  Training Step: 990...  Training loss: 1.9763...  4.5672 sec/batch\n",
      "Epoch: 16/20...  Training Step: 991...  Training loss: 2.0577...  4.7033 sec/batch\n",
      "Epoch: 16/20...  Training Step: 992...  Training loss: 1.9595...  4.5482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 993...  Training loss: 1.8745...  4.5863 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 994...  Training loss: 1.8854...  4.8194 sec/batch\n",
      "Epoch: 16/20...  Training Step: 995...  Training loss: 1.9179...  4.6804 sec/batch\n",
      "Epoch: 16/20...  Training Step: 996...  Training loss: 1.8879...  4.7244 sec/batch\n",
      "Epoch: 16/20...  Training Step: 997...  Training loss: 1.9681...  4.5522 sec/batch\n",
      "Epoch: 16/20...  Training Step: 998...  Training loss: 2.0347...  5.0176 sec/batch\n",
      "Epoch: 16/20...  Training Step: 999...  Training loss: 2.0178...  4.4502 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1000...  Training loss: 1.9626...  4.4882 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1001...  Training loss: 1.9783...  4.5402 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1002...  Training loss: 1.8995...  4.8094 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1003...  Training loss: 1.9494...  4.8114 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1004...  Training loss: 1.9665...  4.9835 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1005...  Training loss: 1.9489...  4.5312 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1006...  Training loss: 1.8938...  4.7864 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1007...  Training loss: 1.9902...  5.1006 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1008...  Training loss: 2.0596...  4.5512 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1009...  Training loss: 2.0125...  4.8745 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1010...  Training loss: 1.9366...  5.4128 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1011...  Training loss: 1.9193...  4.8915 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1012...  Training loss: 1.9187...  4.6123 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1013...  Training loss: 1.9749...  4.8274 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1014...  Training loss: 1.9493...  4.7824 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1015...  Training loss: 1.9153...  4.8334 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1016...  Training loss: 2.0169...  4.7584 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1017...  Training loss: 1.9364...  4.6683 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1018...  Training loss: 2.0345...  4.7174 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1019...  Training loss: 1.9565...  4.8424 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1020...  Training loss: 1.9126...  4.8274 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1021...  Training loss: 1.9475...  4.6073 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1022...  Training loss: 1.9643...  5.2237 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1023...  Training loss: 1.9775...  5.0986 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1024...  Training loss: 1.9251...  4.9185 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1025...  Training loss: 1.9949...  4.6553 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1026...  Training loss: 1.9606...  4.7053 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1027...  Training loss: 1.9142...  4.7164 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1028...  Training loss: 1.9527...  4.6923 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1029...  Training loss: 1.9596...  4.6873 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1030...  Training loss: 1.8651...  4.6513 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1031...  Training loss: 1.9598...  4.7424 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1032...  Training loss: 1.9578...  5.1947 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1033...  Training loss: 1.9425...  4.6283 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1034...  Training loss: 1.8853...  4.8344 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1035...  Training loss: 1.9472...  4.6823 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1036...  Training loss: 1.9708...  4.6723 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1037...  Training loss: 1.9963...  4.7224 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1038...  Training loss: 1.9688...  4.8054 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1039...  Training loss: 1.9759...  4.9405 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1040...  Training loss: 1.9437...  4.7554 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1041...  Training loss: 1.9291...  4.7394 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1042...  Training loss: 1.9787...  4.6833 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1043...  Training loss: 1.9681...  4.6813 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1044...  Training loss: 1.9621...  4.6743 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1045...  Training loss: 1.9498...  4.7154 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1046...  Training loss: 1.9583...  4.6223 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1047...  Training loss: 1.9023...  4.6973 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1048...  Training loss: 1.9878...  4.7474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1049...  Training loss: 1.9122...  4.6803 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1050...  Training loss: 1.8801...  4.4682 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1051...  Training loss: 1.8828...  4.7103 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1052...  Training loss: 1.9752...  4.6353 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1053...  Training loss: 1.9518...  5.0576 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1054...  Training loss: 1.8896...  4.7113 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1055...  Training loss: 1.9352...  4.4291 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1056...  Training loss: 1.9484...  4.8034 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1057...  Training loss: 2.0417...  4.5722 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1058...  Training loss: 1.9399...  4.5893 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1059...  Training loss: 1.8517...  4.8595 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1060...  Training loss: 1.8711...  5.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1061...  Training loss: 1.9114...  4.6103 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1062...  Training loss: 1.8663...  4.9976 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1063...  Training loss: 1.9330...  4.6123 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1064...  Training loss: 2.0200...  4.5813 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1065...  Training loss: 1.9992...  4.6503 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1066...  Training loss: 1.9543...  4.7023 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1067...  Training loss: 1.9824...  4.7884 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1068...  Training loss: 1.8690...  4.6183 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1069...  Training loss: 1.9273...  4.6013 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1070...  Training loss: 1.9402...  4.5993 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1071...  Training loss: 1.9160...  4.7164 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1072...  Training loss: 1.8683...  4.6453 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1073...  Training loss: 1.9718...  4.9145 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1074...  Training loss: 2.0276...  4.8014 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1075...  Training loss: 1.9750...  4.8184 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1076...  Training loss: 1.9155...  4.5282 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1077...  Training loss: 1.9038...  4.7053 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1078...  Training loss: 1.9050...  4.7474 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1079...  Training loss: 1.9484...  4.7454 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1080...  Training loss: 1.9201...  4.6063 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1081...  Training loss: 1.9078...  4.6883 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1082...  Training loss: 1.9859...  4.6973 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1083...  Training loss: 1.9186...  4.6833 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1084...  Training loss: 2.0189...  4.6803 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1085...  Training loss: 1.9579...  4.7834 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1086...  Training loss: 1.8963...  4.6443 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1087...  Training loss: 1.9332...  4.5883 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1088...  Training loss: 1.9501...  4.8114 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1089...  Training loss: 1.9579...  4.5602 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1090...  Training loss: 1.9160...  4.6643 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1091...  Training loss: 1.9621...  4.9255 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 1092...  Training loss: 1.9409...  4.5622 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1093...  Training loss: 1.9071...  4.7033 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1094...  Training loss: 1.9504...  4.6253 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1095...  Training loss: 1.9360...  4.9085 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1096...  Training loss: 1.8356...  4.5732 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1097...  Training loss: 1.9328...  4.6823 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1098...  Training loss: 1.9235...  4.6723 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1099...  Training loss: 1.9251...  4.6503 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1100...  Training loss: 1.8443...  4.8174 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1101...  Training loss: 1.8967...  4.5362 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1102...  Training loss: 1.9647...  4.8374 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1103...  Training loss: 1.9642...  4.7204 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1104...  Training loss: 1.9436...  4.6353 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1105...  Training loss: 1.9602...  4.7444 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1106...  Training loss: 1.9174...  4.7844 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1107...  Training loss: 1.9097...  4.6013 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1108...  Training loss: 1.9550...  4.7484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1109...  Training loss: 1.9429...  4.9295 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1110...  Training loss: 1.9314...  4.6003 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1111...  Training loss: 1.9180...  4.7304 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1112...  Training loss: 1.9421...  5.1166 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1113...  Training loss: 1.8640...  4.5532 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1114...  Training loss: 1.9377...  4.5342 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1115...  Training loss: 1.8749...  4.7274 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1116...  Training loss: 1.8763...  4.4412 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1117...  Training loss: 1.8574...  4.7874 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1118...  Training loss: 1.9374...  4.8444 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1119...  Training loss: 1.9208...  4.6613 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1120...  Training loss: 1.8790...  4.6313 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1121...  Training loss: 1.9221...  4.6593 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1122...  Training loss: 1.9212...  4.7994 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1123...  Training loss: 2.0104...  4.7254 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1124...  Training loss: 1.9159...  5.2567 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1125...  Training loss: 1.8525...  4.9465 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1126...  Training loss: 1.8334...  4.6963 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1127...  Training loss: 1.8763...  4.6883 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1128...  Training loss: 1.8470...  4.6293 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1129...  Training loss: 1.9154...  4.7194 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1130...  Training loss: 1.9791...  4.5913 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1131...  Training loss: 1.9612...  4.8104 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1132...  Training loss: 1.9051...  4.6733 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1133...  Training loss: 1.9381...  4.9925 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1134...  Training loss: 1.8600...  4.7484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1135...  Training loss: 1.9031...  4.4231 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1136...  Training loss: 1.9196...  4.6773 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1137...  Training loss: 1.8798...  4.9795 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1138...  Training loss: 1.8612...  4.9345 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1139...  Training loss: 1.9594...  4.5202 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1140...  Training loss: 2.0026...  4.6813 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1141...  Training loss: 1.9544...  4.7814 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1142...  Training loss: 1.8943...  4.6273 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1143...  Training loss: 1.8809...  4.7073 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1144...  Training loss: 1.8835...  4.6843 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1145...  Training loss: 1.9247...  4.7794 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1146...  Training loss: 1.8931...  4.4832 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1147...  Training loss: 1.8728...  4.8695 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1148...  Training loss: 1.9540...  4.6223 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1149...  Training loss: 1.8995...  4.5342 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1150...  Training loss: 1.9905...  4.8194 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1151...  Training loss: 1.9147...  4.6523 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1152...  Training loss: 1.8802...  4.7023 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1153...  Training loss: 1.9150...  4.6503 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1154...  Training loss: 1.9080...  4.8625 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1155...  Training loss: 1.9457...  4.4832 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1156...  Training loss: 1.8856...  4.5212 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1157...  Training loss: 1.9506...  4.5903 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1158...  Training loss: 1.9188...  4.7214 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1159...  Training loss: 1.8807...  4.6273 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1160...  Training loss: 1.9189...  4.6473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1161...  Training loss: 1.9081...  4.8064 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1162...  Training loss: 1.8142...  4.7154 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1163...  Training loss: 1.9150...  4.7340 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1164...  Training loss: 1.8934...  4.6293 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1165...  Training loss: 1.8822...  4.7884 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1166...  Training loss: 1.8317...  4.7624 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1167...  Training loss: 1.8838...  4.6713 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1168...  Training loss: 1.9295...  4.9625 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1169...  Training loss: 1.9477...  4.5122 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1170...  Training loss: 1.9102...  4.7484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1171...  Training loss: 1.9475...  4.5722 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1172...  Training loss: 1.8913...  4.8585 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1173...  Training loss: 1.8882...  4.6223 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1174...  Training loss: 1.9318...  4.8024 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1175...  Training loss: 1.9211...  4.6053 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1176...  Training loss: 1.9278...  4.7234 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1177...  Training loss: 1.9111...  4.7954 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1178...  Training loss: 1.9303...  4.7474 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1179...  Training loss: 1.8424...  4.6473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1180...  Training loss: 1.9041...  4.5182 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1181...  Training loss: 1.8527...  4.9065 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1182...  Training loss: 1.8497...  5.0286 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1183...  Training loss: 1.8367...  4.6963 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1184...  Training loss: 1.9178...  4.7894 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1185...  Training loss: 1.9100...  4.7424 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1186...  Training loss: 1.8460...  5.2167 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1187...  Training loss: 1.8962...  5.0016 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1188...  Training loss: 1.9088...  4.6703 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1189...  Training loss: 1.9806...  4.5692 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 1190...  Training loss: 1.8720...  4.7214 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1191...  Training loss: 1.8256...  4.6873 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1192...  Training loss: 1.8053...  4.6693 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1193...  Training loss: 1.8586...  4.5723 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1194...  Training loss: 1.8142...  4.6643 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1195...  Training loss: 1.8875...  4.7684 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1196...  Training loss: 1.9708...  4.8414 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1197...  Training loss: 1.9505...  4.5412 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1198...  Training loss: 1.9116...  4.5492 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1199...  Training loss: 1.9256...  4.5993 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1200...  Training loss: 1.8248...  4.7944 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1201...  Training loss: 1.8923...  4.3831 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1202...  Training loss: 1.9076...  4.8074 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1203...  Training loss: 1.8604...  4.6423 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1204...  Training loss: 1.8396...  4.6263 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1205...  Training loss: 1.9292...  4.7704 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1206...  Training loss: 1.9948...  4.7774 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1207...  Training loss: 1.9481...  4.6703 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1208...  Training loss: 1.8768...  4.5632 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1209...  Training loss: 1.8614...  4.5042 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1210...  Training loss: 1.8428...  4.6783 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1211...  Training loss: 1.8961...  4.6083 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1212...  Training loss: 1.8652...  4.6693 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1213...  Training loss: 1.8633...  4.7324 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1214...  Training loss: 1.9303...  4.8875 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1215...  Training loss: 1.8833...  4.8244 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1216...  Training loss: 1.9609...  4.6183 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1217...  Training loss: 1.9149...  4.6383 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1218...  Training loss: 1.8550...  4.6463 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1219...  Training loss: 1.8869...  4.6883 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1220...  Training loss: 1.8962...  4.7994 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1221...  Training loss: 1.9172...  4.6483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1222...  Training loss: 1.8512...  4.5472 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1223...  Training loss: 1.9157...  4.5312 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1224...  Training loss: 1.8911...  4.7744 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1225...  Training loss: 1.8524...  4.6613 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1226...  Training loss: 1.8869...  4.6183 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1227...  Training loss: 1.8900...  4.7584 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1228...  Training loss: 1.7869...  4.5272 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1229...  Training loss: 1.8899...  4.5182 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1230...  Training loss: 1.8901...  4.7023 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1231...  Training loss: 1.8876...  4.9315 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1232...  Training loss: 1.8156...  4.5863 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1233...  Training loss: 1.8465...  4.8434 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1234...  Training loss: 1.9010...  4.8154 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1235...  Training loss: 1.8971...  4.6123 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1236...  Training loss: 1.9109...  4.5402 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1237...  Training loss: 1.9029...  4.6633 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1238...  Training loss: 1.8490...  4.6883 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1239...  Training loss: 1.8585...  4.7284 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1240...  Training loss: 1.9184...  4.6163 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1241...  Training loss: 1.8847...  4.8735 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1242...  Training loss: 1.9105...  4.5903 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1243...  Training loss: 1.8859...  5.1186 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1244...  Training loss: 1.9199...  4.5853 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1245...  Training loss: 1.8168...  4.7414 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1246...  Training loss: 1.8825...  4.8655 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1247...  Training loss: 1.8250...  4.5442 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1248...  Training loss: 1.8370...  4.8244 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1249...  Training loss: 1.8343...  4.7534 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1250...  Training loss: 1.9197...  4.5592 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1251...  Training loss: 1.8849...  5.3438 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1252...  Training loss: 1.8438...  4.6883 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1253...  Training loss: 1.8648...  4.6933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 1254...  Training loss: 1.8789...  4.6793 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1255...  Training loss: 1.9509...  4.6193 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1256...  Training loss: 1.8611...  5.0956 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1257...  Training loss: 1.8074...  4.5822 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1258...  Training loss: 1.8051...  4.4982 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1259...  Training loss: 1.8299...  4.6523 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1260...  Training loss: 1.8206...  5.0206 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1261...  Training loss: 1.8371...  4.6633 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1262...  Training loss: 1.9525...  4.5943 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1263...  Training loss: 1.9164...  4.6383 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1264...  Training loss: 1.8517...  4.6303 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1265...  Training loss: 1.8903...  4.6863 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1266...  Training loss: 1.8043...  4.8194 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1267...  Training loss: 1.8512...  4.6683 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1268...  Training loss: 1.8759...  4.6093 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1269...  Training loss: 1.8371...  4.8484 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1270...  Training loss: 1.8207...  4.4482 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1271...  Training loss: 1.9134...  4.6733 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1272...  Training loss: 1.9674...  4.6563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1273...  Training loss: 1.9292...  4.6603 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1274...  Training loss: 1.8587...  4.8324 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1275...  Training loss: 1.8641...  4.8544 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1276...  Training loss: 1.8292...  4.8014 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1277...  Training loss: 1.8989...  4.5402 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1278...  Training loss: 1.8437...  4.6433 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1279...  Training loss: 1.8341...  4.5632 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1280...  Training loss: 1.8994...  4.9115 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1281...  Training loss: 1.8534...  4.7224 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1282...  Training loss: 1.9418...  4.9105 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1283...  Training loss: 1.8944...  4.4712 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1284...  Training loss: 1.8392...  4.6643 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1285...  Training loss: 1.8694...  4.6363 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1286...  Training loss: 1.8653...  4.5702 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1287...  Training loss: 1.8867...  4.6193 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 1288...  Training loss: 1.8483...  4.8434 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1289...  Training loss: 1.8919...  4.7043 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1290...  Training loss: 1.8613...  4.6693 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1291...  Training loss: 1.8351...  5.0866 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1292...  Training loss: 1.8934...  4.6943 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1293...  Training loss: 1.8588...  4.6813 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1294...  Training loss: 1.7642...  4.7954 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1295...  Training loss: 1.8627...  4.7354 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1296...  Training loss: 1.8492...  4.7144 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1297...  Training loss: 1.8609...  4.7174 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1298...  Training loss: 1.8119...  4.6893 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1299...  Training loss: 1.8252...  5.3398 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1300...  Training loss: 1.8913...  4.6783 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1301...  Training loss: 1.9026...  4.6153 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1302...  Training loss: 1.8695...  4.6493 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1303...  Training loss: 1.8867...  4.8164 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1304...  Training loss: 1.8546...  4.7003 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1305...  Training loss: 1.8476...  4.6563 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1306...  Training loss: 1.8807...  4.8534 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1307...  Training loss: 1.8745...  4.8244 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1308...  Training loss: 1.8782...  4.5032 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1309...  Training loss: 1.8706...  4.5372 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1310...  Training loss: 1.8748...  4.7053 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1311...  Training loss: 1.8169...  4.7063 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1312...  Training loss: 1.8542...  4.5502 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1313...  Training loss: 1.8137...  4.6483 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1314...  Training loss: 1.8040...  4.7514 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1315...  Training loss: 1.8098...  5.0676 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1316...  Training loss: 1.8993...  4.6953 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1317...  Training loss: 1.8554...  4.5462 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1318...  Training loss: 1.8099...  4.9585 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1319...  Training loss: 1.8512...  4.7874 sec/batch\n",
      "Epoch: 20/20...  Training Step: 1320...  Training loss: 1.8539...  4.6653 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Сохраняться каждый N итераций\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Можно раскомментировать строчку ниже и продолжить обучение с checkpoint'а\n",
    "   # saver.restore(sess, 'checkpoints/i320_l512.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Обучаем сеть\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"Коли втрачаю я рівновагу, \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1320_l512.ckpt\n",
      "Коли втрачаю я рівновагу, яка не падеравовий споста,\n",
      "     запле наси те замути\n",
      "     простою\n",
      "     на поростути триваєш на зному\n",
      "     і насілом на прості,\n",
      "     з витрами свіганий мастина.\n",
      "     Все, що в ньоми з тима приходить світти,\n",
      "     забивали на всій сверів на вісту\n",
      "     і ступали,\n",
      "     і в насі, мов висторито залости.\n",
      "     І нічерні сталі помарилися, ми в тенбомих\n",
      "     іністи, що теревона відчу вона води.\n",
      "     Вироз на все не повани пем’ять – пробовнаю,\n",
      "     зі своїю толом,\n",
      "     і стіраюти повертою в кількими не викали толки,\n",
      "     і все, що потинею в нібо з таких наволонамі.\n",
      "     \n",
      "     Помінники мастіманими від нами\n",
      "     подитуєш сорці не від небі золонне,\n",
      "     нач скалить витись, як від віч вого на звори,\n",
      "     і вини повітряла старі,\n",
      "     наси всі сворина.\n",
      "     \n",
      "     \n",
      "     2001 \n",
      "     \n",
      "\n",
      "      \n",
      "      Поментрів дитяває стирі на знасти в свогому\n",
      "  о  вікнала стого потравня, як нерідну вітро,\n",
      "     всім при содцех морто –\n",
      "     всі тебе в тамої витинну\n",
      "     перестори в тиму відкили, на привене з нестому і собою\n",
      "     із наше з сихається знів,\n",
      "     над тобувати на спородних підем.\n",
      "     \n",
      "     В семі ні сторів – станах не зводому ворну\n",
      "     по заморитиму, ще в пісталих повоним залицих,\n",
      "     ількі прасові свільких привиховних слухивалонок.\n",
      "     \n",
      "     І нобру сторіти сомі, ков стари,\n",
      "     я все за страму претові,\n",
      "     залиши в тробу, небову,\n",
      "     якого виришується скудів,\n",
      "     і тровенний споваті,\n",
      "     я на виростають потого залові\n",
      "     прості паравіть в тепелемий.\n",
      "     \n",
      "     І померта спроби на може тикий в н\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1320_l512.ckpt'\n",
    "samp = sample(checkpoint, 1500, lstm_size, len(vocab))\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
